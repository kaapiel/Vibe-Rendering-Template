//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21112126
// Cuda compilation tools, release 8.0, V8.0.43
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_30
.address_size 64

	// .globl	ThresholdLookupAlphaKernel
.global .texref inRGBALUT;
.global .texref inMLUT;
.const .align 4 .b8 kRGB32f_To_601YPbPr[36] = {135, 22, 153, 62, 162, 69, 22, 63, 213, 120, 233, 61, 33, 201, 44, 190, 111, 155, 169, 190, 0, 0, 0, 63, 0, 0, 0, 63, 70, 94, 214, 190, 232, 134, 166, 189};
.const .align 4 .b8 k601YPbPr_To_RGB32f[36] = {0, 0, 128, 63, 0, 0, 0, 0, 188, 116, 179, 63, 0, 0, 128, 63, 152, 50, 176, 190, 158, 209, 54, 191, 0, 0, 128, 63, 229, 208, 226, 63, 0, 0, 0, 0};
.const .align 4 .b8 kRGB32f_To_601YCbCr[36] = {70, 246, 130, 66, 145, 141, 0, 67, 94, 186, 199, 65, 33, 48, 23, 194, 240, 103, 148, 194, 0, 0, 224, 66, 0, 0, 224, 66, 111, 146, 187, 194, 70, 182, 145, 193};
.const .align 4 .b8 k601YCbCr_To_RGB32f[36] = {37, 160, 149, 59, 0, 0, 0, 0, 182, 23, 205, 59, 37, 160, 149, 59, 40, 15, 201, 186, 156, 239, 80, 187, 37, 160, 149, 59, 236, 155, 1, 60, 0, 0, 0, 0};
.const .align 4 .b8 kRGB8u_To_601YCbCr[36] = {219, 121, 131, 62, 152, 14, 1, 63, 18, 131, 200, 61, 174, 199, 23, 190, 238, 252, 148, 190, 197, 224, 224, 62, 197, 224, 224, 62, 217, 78, 188, 190, 174, 71, 146, 189};
.const .align 4 .b8 k601YCbCr_To_RGB8u[36] = {127, 10, 149, 63, 0, 0, 0, 0, 160, 74, 204, 63, 127, 10, 149, 63, 254, 148, 200, 190, 184, 30, 80, 191, 127, 10, 149, 63, 78, 26, 1, 64, 0, 0, 0, 0};
.const .align 4 .b8 kRGB8u_To_601YCbCrFullRange[36] = {135, 22, 153, 62, 162, 69, 22, 63, 213, 120, 233, 61, 166, 27, 44, 190, 39, 241, 168, 190, 250, 254, 254, 62, 250, 254, 254, 62, 43, 135, 213, 190, 59, 223, 165, 189};
.const .align 4 .b8 k601YCbCrFullRange_To_RGB8u[36] = {0, 0, 128, 63, 0, 0, 0, 0, 72, 193, 178, 63, 0, 0, 128, 63, 143, 130, 175, 190, 225, 26, 54, 191, 0, 0, 128, 63, 20, 238, 225, 63, 0, 0, 0, 0};
.const .align 4 .b8 kRGB32f_To_601YCbCrFullRange[36] = {113, 125, 152, 66, 92, 175, 21, 67, 92, 143, 232, 65, 158, 111, 43, 194, 49, 72, 168, 194, 0, 0, 254, 66, 0, 0, 254, 66, 170, 177, 212, 194, 88, 57, 165, 193};
.const .align 4 .b8 k601YCbCrFullRange_To_RGB32f[36] = {129, 128, 128, 59, 0, 0, 0, 0, 188, 116, 179, 59, 129, 128, 128, 59, 194, 50, 176, 186, 179, 209, 54, 187, 129, 128, 128, 59, 229, 208, 226, 59, 0, 0, 0, 0};
.const .align 4 .b8 kRGB32f_To_709YPbPr[36] = {208, 179, 89, 62, 89, 23, 55, 63, 152, 221, 147, 61, 186, 164, 234, 189, 210, 86, 197, 190, 0, 0, 0, 63, 0, 0, 0, 63, 190, 134, 232, 190, 16, 202, 59, 189};
.const .align 4 .b8 k709YPbPr_To_RGB32f[36] = {0, 0, 128, 63, 0, 0, 0, 0, 12, 147, 201, 63, 0, 0, 128, 63, 221, 209, 63, 190, 243, 173, 239, 190, 0, 0, 128, 63, 77, 132, 237, 63, 0, 0, 0, 0};
.const .align 4 .b8 kRGB32f_To_709YCbCr[36] = {106, 60, 58, 66, 6, 161, 28, 67, 244, 253, 124, 65, 223, 79, 205, 193, 8, 172, 172, 194, 0, 0, 224, 66, 0, 0, 224, 66, 195, 117, 203, 194, 236, 81, 36, 193};
.const .align 4 .b8 k709YCbCr_To_RGB32f[36] = {37, 160, 149, 59, 0, 0, 0, 0, 239, 94, 230, 59, 37, 160, 149, 59, 33, 57, 91, 186, 178, 245, 8, 187, 37, 160, 149, 59, 82, 185, 7, 60, 0, 0, 0, 0};
.const .align 4 .b8 k709YCbCrFullRange_To_RGB32f[36] = {131, 128, 128, 59, 0, 0, 0, 0, 28, 147, 201, 59, 131, 128, 128, 59, 61, 210, 63, 186, 248, 173, 239, 186, 131, 128, 128, 59, 82, 132, 237, 59, 0, 0, 0, 0};
.const .align 4 .b8 kRGB8u_To_709YCbCr[36] = {207, 247, 58, 62, 53, 62, 29, 63, 231, 251, 125, 61, 184, 30, 206, 189, 23, 89, 173, 190, 197, 224, 224, 62, 197, 224, 224, 62, 12, 66, 204, 190, 195, 245, 36, 189};
.const .align 4 .b8 k709YCbCr_To_RGB8u[36] = {127, 10, 149, 63, 0, 0, 0, 0, 147, 120, 229, 63, 127, 10, 149, 63, 53, 94, 90, 190, 205, 108, 8, 191, 127, 10, 149, 63, 154, 49, 7, 64, 0, 0, 0, 0};
.const .align 4 .b8 k709YCbCr_To_601YCbCr[36] = {0, 0, 128, 63, 23, 100, 203, 61, 1, 77, 68, 62, 0, 0, 0, 0, 18, 103, 125, 63, 10, 158, 226, 189, 0, 0, 0, 0, 61, 98, 148, 189, 249, 191, 123, 63};
.const .align 4 .b8 k601YCbCr_To_709YCbCr[36] = {0, 0, 128, 63, 122, 165, 236, 189, 179, 237, 84, 190, 0, 0, 0, 0, 204, 98, 130, 63, 216, 188, 234, 61, 0, 0, 0, 0, 74, 179, 153, 61, 234, 61, 131, 63};
.const .align 4 .b8 kYCbCrOffset[12] = {0, 0, 128, 65, 0, 0, 0, 67, 0, 0, 0, 67};
.const .align 4 .b8 kYCbCrFullRangeOffset[12] = {0, 0, 0, 0, 0, 0, 0, 67, 0, 0, 0, 67};
.const .align 4 .f32 kMinAlphaValue = 0f24E69595;

.visible .entry ThresholdLookupAlphaKernel(
	.param .u64 ThresholdLookupAlphaKernel_param_0,
	.param .u64 ThresholdLookupAlphaKernel_param_1,
	.param .u32 ThresholdLookupAlphaKernel_param_2,
	.param .u32 ThresholdLookupAlphaKernel_param_3,
	.param .u32 ThresholdLookupAlphaKernel_param_4,
	.param .u32 ThresholdLookupAlphaKernel_param_5,
	.param .u32 ThresholdLookupAlphaKernel_param_6,
	.param .f32 ThresholdLookupAlphaKernel_param_7,
	.param .f32 ThresholdLookupAlphaKernel_param_8,
	.param .u32 ThresholdLookupAlphaKernel_param_9
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<11>;
	.reg .f32 	%f<26>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd3, [ThresholdLookupAlphaKernel_param_0];
	ld.param.u64 	%rd4, [ThresholdLookupAlphaKernel_param_1];
	ld.param.u32 	%r3, [ThresholdLookupAlphaKernel_param_2];
	ld.param.u32 	%r4, [ThresholdLookupAlphaKernel_param_3];
	ld.param.u32 	%r5, [ThresholdLookupAlphaKernel_param_4];
	ld.param.u32 	%r6, [ThresholdLookupAlphaKernel_param_5];
	ld.param.u32 	%r7, [ThresholdLookupAlphaKernel_param_6];
	ld.param.f32 	%f14, [ThresholdLookupAlphaKernel_param_7];
	ld.param.f32 	%f15, [ThresholdLookupAlphaKernel_param_8];
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r8, %r9, %r10;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r11, %r12, %r13;
	setp.lt.s32	%p1, %r1, %r6;
	setp.lt.s32	%p2, %r2, %r7;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB0_7;
	bra.uni 	BB0_1;

BB0_1:
	mad.lo.s32 	%r14, %r2, %r3, %r1;
	cvt.s64.s32	%rd1, %r14;
	setp.eq.s32	%p4, %r5, 0;
	@%p4 bra 	BB0_3;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v4.f32 	{%f16, %f17, %f18, %f19}, [%rd7];
	mov.f32 	%f25, %f19;
	mov.f32 	%f3, %f18;
	mov.f32 	%f2, %f17;
	mov.f32 	%f1, %f16;
	bra.uni 	BB0_4;

BB0_3:
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 3;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd10];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs4;
	cvt.f32.f16 	%f25, %temp;
	}

BB0_4:
	setp.lt.ftz.f32	%p5, %f25, %f14;
	sub.ftz.f32 	%f20, %f25, %f14;
	mul.ftz.f32 	%f21, %f20, %f15;
	selp.f32	%f22, 0f00000000, %f21, %p5;
	setp.gt.ftz.f32	%p6, %f22, 0f3F800000;
	selp.f32	%f13, 0f3F800000, %f22, %p6;
	mad.lo.s32 	%r15, %r2, %r4, %r1;
	cvt.s64.s32	%rd2, %r15;
	@%p4 bra 	BB0_6;

	cvta.to.global.u64 	%rd11, %rd4;
	shl.b64 	%rd12, %rd2, 4;
	add.s64 	%rd13, %rd11, %rd12;
	mov.f32 	%f23, 0f00000000;
	st.global.v4.f32 	[%rd13], {%f23, %f23, %f23, %f13};
	bra.uni 	BB0_7;

BB0_6:
	cvta.to.global.u64 	%rd14, %rd4;
	shl.b64 	%rd15, %rd2, 3;
	add.s64 	%rd16, %rd14, %rd15;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f13;
	mov.b16 	%rs9, %temp;
}
	mov.f32 	%f24, 0f00000000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f24;
	mov.b16 	%rs10, %temp;
}
	st.global.v4.u16 	[%rd16], {%rs10, %rs10, %rs10, %rs9};

BB0_7:
	ret;
}

	// .globl	ThresholdLookupRGBKernel
.visible .entry ThresholdLookupRGBKernel(
	.param .u64 ThresholdLookupRGBKernel_param_0,
	.param .u64 ThresholdLookupRGBKernel_param_1,
	.param .u32 ThresholdLookupRGBKernel_param_2,
	.param .u32 ThresholdLookupRGBKernel_param_3,
	.param .u32 ThresholdLookupRGBKernel_param_4,
	.param .u32 ThresholdLookupRGBKernel_param_5,
	.param .u32 ThresholdLookupRGBKernel_param_6,
	.param .f32 ThresholdLookupRGBKernel_param_7,
	.param .f32 ThresholdLookupRGBKernel_param_8,
	.param .u32 ThresholdLookupRGBKernel_param_9
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<13>;
	.reg .f32 	%f<42>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd3, [ThresholdLookupRGBKernel_param_0];
	ld.param.u64 	%rd4, [ThresholdLookupRGBKernel_param_1];
	ld.param.u32 	%r3, [ThresholdLookupRGBKernel_param_2];
	ld.param.u32 	%r4, [ThresholdLookupRGBKernel_param_3];
	ld.param.u32 	%r5, [ThresholdLookupRGBKernel_param_4];
	ld.param.u32 	%r7, [ThresholdLookupRGBKernel_param_5];
	ld.param.u32 	%r8, [ThresholdLookupRGBKernel_param_6];
	ld.param.f32 	%f23, [ThresholdLookupRGBKernel_param_7];
	ld.param.f32 	%f24, [ThresholdLookupRGBKernel_param_8];
	ld.param.u32 	%r6, [ThresholdLookupRGBKernel_param_9];
	mov.u32 	%r9, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	mov.u32 	%r12, %ntid.y;
	mov.u32 	%r13, %ctaid.y;
	mov.u32 	%r14, %tid.y;
	mad.lo.s32 	%r2, %r12, %r13, %r14;
	setp.lt.s32	%p1, %r1, %r7;
	setp.lt.s32	%p2, %r2, %r8;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB1_9;
	bra.uni 	BB1_1;

BB1_1:
	mad.lo.s32 	%r15, %r2, %r3, %r1;
	cvt.s64.s32	%rd1, %r15;
	setp.eq.s32	%p4, %r5, 0;
	@%p4 bra 	BB1_3;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v4.f32 	{%f25, %f26, %f27, %f28}, [%rd7];
	mov.f32 	%f38, %f28;
	mov.f32 	%f37, %f27;
	mov.f32 	%f36, %f26;
	mov.f32 	%f35, %f25;
	bra.uni 	BB1_4;

BB1_3:
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 3;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd10];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs1;
	cvt.f32.f16 	%f35, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs2;
	cvt.f32.f16 	%f36, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs3;
	cvt.f32.f16 	%f37, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs4;
	cvt.f32.f16 	%f38, %temp;
	}

BB1_4:
	setp.lt.ftz.f32	%p5, %f37, %f23;
	sub.ftz.f32 	%f29, %f37, %f23;
	mul.ftz.f32 	%f30, %f29, %f24;
	selp.f32	%f41, 0f00000000, %f30, %p5;
	setp.lt.ftz.f32	%p6, %f36, %f23;
	sub.ftz.f32 	%f31, %f36, %f23;
	mul.ftz.f32 	%f32, %f31, %f24;
	selp.f32	%f39, 0f00000000, %f32, %p6;
	setp.lt.ftz.f32	%p7, %f35, %f23;
	sub.ftz.f32 	%f33, %f35, %f23;
	mul.ftz.f32 	%f34, %f33, %f24;
	selp.f32	%f40, 0f00000000, %f34, %p7;
	setp.eq.s32	%p8, %r6, 0;
	@%p8 bra 	BB1_6;

	cvt.ftz.sat.f32.f32	%f41, %f41;
	cvt.ftz.sat.f32.f32	%f39, %f39;
	cvt.ftz.sat.f32.f32	%f40, %f40;

BB1_6:
	mad.lo.s32 	%r16, %r2, %r4, %r1;
	cvt.s64.s32	%rd2, %r16;
	@%p4 bra 	BB1_8;

	cvta.to.global.u64 	%rd11, %rd4;
	shl.b64 	%rd12, %rd2, 4;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.v4.f32 	[%rd13], {%f40, %f39, %f41, %f38};
	bra.uni 	BB1_9;

BB1_8:
	cvta.to.global.u64 	%rd14, %rd4;
	shl.b64 	%rd15, %rd2, 3;
	add.s64 	%rd16, %rd14, %rd15;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f38;
	mov.b16 	%rs9, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f41;
	mov.b16 	%rs10, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f39;
	mov.b16 	%rs11, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f40;
	mov.b16 	%rs12, %temp;
}
	st.global.v4.u16 	[%rd16], {%rs12, %rs11, %rs10, %rs9};

BB1_9:
	ret;
}

	// .globl	ThresholdLookupLuminanceKernel
.visible .entry ThresholdLookupLuminanceKernel(
	.param .u64 ThresholdLookupLuminanceKernel_param_0,
	.param .u64 ThresholdLookupLuminanceKernel_param_1,
	.param .u32 ThresholdLookupLuminanceKernel_param_2,
	.param .u32 ThresholdLookupLuminanceKernel_param_3,
	.param .u32 ThresholdLookupLuminanceKernel_param_4,
	.param .u32 ThresholdLookupLuminanceKernel_param_5,
	.param .u32 ThresholdLookupLuminanceKernel_param_6,
	.param .f32 ThresholdLookupLuminanceKernel_param_7,
	.param .f32 ThresholdLookupLuminanceKernel_param_8,
	.param .u32 ThresholdLookupLuminanceKernel_param_9
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<13>;
	.reg .f32 	%f<34>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd3, [ThresholdLookupLuminanceKernel_param_0];
	ld.param.u64 	%rd4, [ThresholdLookupLuminanceKernel_param_1];
	ld.param.u32 	%r3, [ThresholdLookupLuminanceKernel_param_2];
	ld.param.u32 	%r4, [ThresholdLookupLuminanceKernel_param_3];
	ld.param.u32 	%r5, [ThresholdLookupLuminanceKernel_param_4];
	ld.param.u32 	%r6, [ThresholdLookupLuminanceKernel_param_5];
	ld.param.u32 	%r7, [ThresholdLookupLuminanceKernel_param_6];
	ld.param.f32 	%f17, [ThresholdLookupLuminanceKernel_param_7];
	ld.param.f32 	%f18, [ThresholdLookupLuminanceKernel_param_8];
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r8, %r9, %r10;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r11, %r12, %r13;
	setp.lt.s32	%p1, %r1, %r6;
	setp.lt.s32	%p2, %r2, %r7;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB2_7;
	bra.uni 	BB2_1;

BB2_1:
	mad.lo.s32 	%r14, %r2, %r3, %r1;
	cvt.s64.s32	%rd1, %r14;
	setp.eq.s32	%p4, %r5, 0;
	@%p4 bra 	BB2_3;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v4.f32 	{%f19, %f20, %f21, %f22}, [%rd7];
	mov.f32 	%f33, %f22;
	mov.f32 	%f32, %f21;
	mov.f32 	%f31, %f20;
	mov.f32 	%f30, %f19;
	bra.uni 	BB2_4;

BB2_3:
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 3;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd10];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs1;
	cvt.f32.f16 	%f30, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs2;
	cvt.f32.f16 	%f31, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs3;
	cvt.f32.f16 	%f32, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs4;
	cvt.f32.f16 	%f33, %temp;
	}

BB2_4:
	mul.ftz.f32 	%f23, %f31, 0f3F1645A2;
	fma.rn.ftz.f32 	%f24, %f32, 0f3E991687, %f23;
	fma.rn.ftz.f32 	%f25, %f30, 0f3DE978D5, %f24;
	mul.ftz.f32 	%f26, %f33, %f25;
	setp.lt.ftz.f32	%p5, %f26, %f17;
	sub.ftz.f32 	%f27, %f26, %f17;
	mul.ftz.f32 	%f28, %f27, %f18;
	selp.f32	%f29, 0f00000000, %f28, %p5;
	setp.gt.ftz.f32	%p6, %f29, 0f3F800000;
	selp.f32	%f16, 0f3F800000, %f29, %p6;
	mad.lo.s32 	%r15, %r2, %r4, %r1;
	cvt.s64.s32	%rd2, %r15;
	@%p4 bra 	BB2_6;

	cvta.to.global.u64 	%rd11, %rd4;
	shl.b64 	%rd12, %rd2, 4;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.v4.f32 	[%rd13], {%f30, %f31, %f32, %f16};
	bra.uni 	BB2_7;

BB2_6:
	cvta.to.global.u64 	%rd14, %rd4;
	shl.b64 	%rd15, %rd2, 3;
	add.s64 	%rd16, %rd14, %rd15;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f16;
	mov.b16 	%rs9, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f32;
	mov.b16 	%rs10, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f31;
	mov.b16 	%rs11, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f30;
	mov.b16 	%rs12, %temp;
}
	st.global.v4.u16 	[%rd16], {%rs12, %rs11, %rs10, %rs9};

BB2_7:
	ret;
}

	// .globl	ThresholdScaleRGBKernel
.visible .entry ThresholdScaleRGBKernel(
	.param .u64 ThresholdScaleRGBKernel_param_0,
	.param .u64 ThresholdScaleRGBKernel_param_1,
	.param .u32 ThresholdScaleRGBKernel_param_2,
	.param .u32 ThresholdScaleRGBKernel_param_3,
	.param .u32 ThresholdScaleRGBKernel_param_4,
	.param .u32 ThresholdScaleRGBKernel_param_5,
	.param .f32 ThresholdScaleRGBKernel_param_6,
	.param .u32 ThresholdScaleRGBKernel_param_7
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<13>;
	.reg .f32 	%f<35>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [ThresholdScaleRGBKernel_param_0];
	ld.param.u64 	%rd3, [ThresholdScaleRGBKernel_param_1];
	ld.param.u32 	%r3, [ThresholdScaleRGBKernel_param_2];
	ld.param.u32 	%r4, [ThresholdScaleRGBKernel_param_3];
	ld.param.u32 	%r6, [ThresholdScaleRGBKernel_param_4];
	ld.param.u32 	%r7, [ThresholdScaleRGBKernel_param_5];
	ld.param.f32 	%f23, [ThresholdScaleRGBKernel_param_6];
	ld.param.u32 	%r5, [ThresholdScaleRGBKernel_param_7];
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r8, %r9, %r10;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r11, %r12, %r13;
	setp.lt.s32	%p1, %r1, %r6;
	setp.lt.s32	%p2, %r2, %r7;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB3_9;
	bra.uni 	BB3_1;

BB3_1:
	mad.lo.s32 	%r14, %r2, %r3, %r1;
	cvt.s64.s32	%rd1, %r14;
	setp.eq.s32	%p4, %r4, 0;
	@%p4 bra 	BB3_3;

	cvta.to.global.u64 	%rd4, %rd2;
	shl.b64 	%rd5, %rd1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v4.f32 	{%f24, %f25, %f26, %f27}, [%rd6];
	mov.f32 	%f31, %f27;
	mov.f32 	%f30, %f26;
	mov.f32 	%f29, %f25;
	mov.f32 	%f28, %f24;
	bra.uni 	BB3_4;

BB3_3:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd9];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs1;
	cvt.f32.f16 	%f28, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs2;
	cvt.f32.f16 	%f29, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs3;
	cvt.f32.f16 	%f30, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs4;
	cvt.f32.f16 	%f31, %temp;
	}

BB3_4:
	mul.ftz.f32 	%f34, %f30, %f23;
	mul.ftz.f32 	%f32, %f29, %f23;
	mul.ftz.f32 	%f33, %f28, %f23;
	setp.eq.s32	%p5, %r5, 0;
	@%p5 bra 	BB3_6;

	cvt.ftz.sat.f32.f32	%f34, %f34;
	cvt.ftz.sat.f32.f32	%f32, %f32;
	cvt.ftz.sat.f32.f32	%f33, %f33;

BB3_6:
	@%p4 bra 	BB3_8;

	cvta.to.global.u64 	%rd10, %rd3;
	shl.b64 	%rd11, %rd1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.v4.f32 	[%rd12], {%f33, %f32, %f34, %f31};
	bra.uni 	BB3_9;

BB3_8:
	cvta.to.global.u64 	%rd13, %rd3;
	shl.b64 	%rd14, %rd1, 3;
	add.s64 	%rd15, %rd13, %rd14;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f31;
	mov.b16 	%rs9, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f34;
	mov.b16 	%rs10, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f32;
	mov.b16 	%rs11, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f33;
	mov.b16 	%rs12, %temp;
}
	st.global.v4.u16 	[%rd15], {%rs12, %rs11, %rs10, %rs9};

BB3_9:
	ret;
}

	// .globl	ThresholdArbTableKernel
.visible .entry ThresholdArbTableKernel(
	.param .u64 ThresholdArbTableKernel_param_0,
	.param .u64 ThresholdArbTableKernel_param_1,
	.param .u64 ThresholdArbTableKernel_param_2,
	.param .u64 ThresholdArbTableKernel_param_3,
	.param .u32 ThresholdArbTableKernel_param_4,
	.param .u32 ThresholdArbTableKernel_param_5,
	.param .u32 ThresholdArbTableKernel_param_6,
	.param .u32 ThresholdArbTableKernel_param_7
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<13>;
	.reg .f32 	%f<46>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd2, [ThresholdArbTableKernel_param_0];
	ld.param.u64 	%rd3, [ThresholdArbTableKernel_param_3];
	ld.param.u32 	%r3, [ThresholdArbTableKernel_param_4];
	ld.param.u32 	%r4, [ThresholdArbTableKernel_param_5];
	ld.param.u32 	%r5, [ThresholdArbTableKernel_param_6];
	ld.param.u32 	%r6, [ThresholdArbTableKernel_param_7];
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r10, %r11, %r12;
	setp.lt.s32	%p1, %r1, %r5;
	setp.lt.s32	%p2, %r2, %r6;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB4_7;
	bra.uni 	BB4_1;

BB4_1:
	mad.lo.s32 	%r13, %r2, %r3, %r1;
	cvt.s64.s32	%rd1, %r13;
	setp.eq.s32	%p4, %r4, 0;
	@%p4 bra 	BB4_3;

	cvta.to.global.u64 	%rd4, %rd2;
	shl.b64 	%rd5, %rd1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v4.f32 	{%f17, %f18, %f19, %f20}, [%rd6];
	mov.f32 	%f45, %f20;
	mov.f32 	%f3, %f19;
	mov.f32 	%f2, %f18;
	mov.f32 	%f1, %f17;
	bra.uni 	BB4_4;

BB4_3:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd9];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs4;
	cvt.f32.f16 	%f45, %temp;
	}

BB4_4:
	fma.rn.ftz.f32 	%f21, %f45, 0f437F0000, 0f3F000000;
	mov.f32 	%f22, 0f3F000000;
	tex.2d.v4.f32.f32	{%f23, %f24, %f25, %f26}, [inRGBALUT, {%f21, %f22}];
	mul.ftz.f32 	%f13, %f26, 0f3B808081;
	mul.ftz.f32 	%f27, %f25, 0f3B808081;
	fma.rn.ftz.f32 	%f28, %f27, 0f437F0000, 0f3F000000;
	mul.ftz.f32 	%f29, %f24, 0f3B808081;
	fma.rn.ftz.f32 	%f30, %f29, 0f437F0000, 0f3F000000;
	mul.ftz.f32 	%f31, %f23, 0f3B808081;
	fma.rn.ftz.f32 	%f32, %f31, 0f437F0000, 0f3F000000;
	tex.2d.v4.f32.f32	{%f33, %f34, %f35, %f36}, [inMLUT, {%f28, %f22}];
	tex.2d.v4.f32.f32	{%f37, %f38, %f39, %f40}, [inMLUT, {%f30, %f22}];
	tex.2d.v4.f32.f32	{%f41, %f42, %f43, %f44}, [inMLUT, {%f32, %f22}];
	mul.ftz.f32 	%f14, %f33, 0f3B808081;
	mul.ftz.f32 	%f15, %f37, 0f3B808081;
	mul.ftz.f32 	%f16, %f41, 0f3B808081;
	@%p4 bra 	BB4_6;

	cvta.to.global.u64 	%rd12, %rd3;
	shl.b64 	%rd13, %rd1, 4;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.v4.f32 	[%rd14], {%f16, %f15, %f14, %f13};
	bra.uni 	BB4_7;

BB4_6:
	cvta.to.global.u64 	%rd15, %rd3;
	shl.b64 	%rd16, %rd1, 3;
	add.s64 	%rd17, %rd15, %rd16;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f13;
	mov.b16 	%rs9, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f14;
	mov.b16 	%rs10, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f15;
	mov.b16 	%rs11, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f16;
	mov.b16 	%rs12, %temp;
}
	st.global.v4.u16 	[%rd17], {%rs12, %rs11, %rs10, %rs9};

BB4_7:
	ret;
}

	// .globl	GlowMultAlphaKernel
.visible .entry GlowMultAlphaKernel(
	.param .u64 GlowMultAlphaKernel_param_0,
	.param .u64 GlowMultAlphaKernel_param_1,
	.param .u64 GlowMultAlphaKernel_param_2,
	.param .u32 GlowMultAlphaKernel_param_3,
	.param .u32 GlowMultAlphaKernel_param_4,
	.param .u32 GlowMultAlphaKernel_param_5,
	.param .u32 GlowMultAlphaKernel_param_6,
	.param .u32 GlowMultAlphaKernel_param_7
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<21>;
	.reg .f32 	%f<42>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd2, [GlowMultAlphaKernel_param_0];
	ld.param.u64 	%rd3, [GlowMultAlphaKernel_param_1];
	ld.param.u64 	%rd4, [GlowMultAlphaKernel_param_2];
	ld.param.u32 	%r3, [GlowMultAlphaKernel_param_3];
	ld.param.u32 	%r4, [GlowMultAlphaKernel_param_4];
	ld.param.u32 	%r5, [GlowMultAlphaKernel_param_5];
	ld.param.u32 	%r6, [GlowMultAlphaKernel_param_6];
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r10, %r11, %r12;
	setp.lt.s32	%p1, %r1, %r5;
	setp.lt.s32	%p2, %r2, %r6;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB5_10;
	bra.uni 	BB5_1;

BB5_1:
	mad.lo.s32 	%r13, %r2, %r3, %r1;
	cvt.s64.s32	%rd1, %r13;
	setp.eq.s32	%p4, %r4, 0;
	@%p4 bra 	BB5_3;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v4.f32 	{%f29, %f30, %f31, %f32}, [%rd7];
	mov.f32 	%f40, %f32;
	mov.f32 	%f39, %f31;
	mov.f32 	%f38, %f30;
	mov.f32 	%f37, %f29;
	bra.uni 	BB5_4;

BB5_3:
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 3;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd10];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs1;
	cvt.f32.f16 	%f37, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs2;
	cvt.f32.f16 	%f38, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs3;
	cvt.f32.f16 	%f39, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs4;
	cvt.f32.f16 	%f40, %temp;
	}

BB5_4:
	@%p4 bra 	BB5_6;

	cvta.to.global.u64 	%rd11, %rd2;
	shl.b64 	%rd12, %rd1, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd13];
	mov.f32 	%f41, %f36;
	mov.f32 	%f15, %f35;
	mov.f32 	%f14, %f34;
	mov.f32 	%f13, %f33;
	bra.uni 	BB5_7;

BB5_6:
	cvta.to.global.u64 	%rd14, %rd2;
	shl.b64 	%rd15, %rd1, 3;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [%rd16];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs12;
	cvt.f32.f16 	%f41, %temp;
	}

BB5_7:
	mul.ftz.f32 	%f28, %f40, %f41;
	@%p4 bra 	BB5_9;

	cvta.to.global.u64 	%rd17, %rd4;
	shl.b64 	%rd18, %rd1, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.v4.f32 	[%rd19], {%f37, %f38, %f39, %f28};
	bra.uni 	BB5_10;

BB5_9:
	cvta.to.global.u64 	%rd20, %rd4;
	shl.b64 	%rd21, %rd1, 3;
	add.s64 	%rd22, %rd20, %rd21;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f28;
	mov.b16 	%rs17, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f39;
	mov.b16 	%rs18, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f38;
	mov.b16 	%rs19, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f37;
	mov.b16 	%rs20, %temp;
}
	st.global.v4.u16 	[%rd22], {%rs20, %rs19, %rs18, %rs17};

BB5_10:
	ret;
}

	// .globl	GlowMultAlphaLumaKernel
.visible .entry GlowMultAlphaLumaKernel(
	.param .u64 GlowMultAlphaLumaKernel_param_0,
	.param .u64 GlowMultAlphaLumaKernel_param_1,
	.param .u64 GlowMultAlphaLumaKernel_param_2,
	.param .u32 GlowMultAlphaLumaKernel_param_3,
	.param .u32 GlowMultAlphaLumaKernel_param_4,
	.param .u32 GlowMultAlphaLumaKernel_param_5,
	.param .u32 GlowMultAlphaLumaKernel_param_6,
	.param .u32 GlowMultAlphaLumaKernel_param_7
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<21>;
	.reg .f32 	%f<57>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd2, [GlowMultAlphaLumaKernel_param_0];
	ld.param.u64 	%rd3, [GlowMultAlphaLumaKernel_param_1];
	ld.param.u64 	%rd4, [GlowMultAlphaLumaKernel_param_2];
	ld.param.u32 	%r3, [GlowMultAlphaLumaKernel_param_3];
	ld.param.u32 	%r4, [GlowMultAlphaLumaKernel_param_4];
	ld.param.u32 	%r5, [GlowMultAlphaLumaKernel_param_5];
	ld.param.u32 	%r6, [GlowMultAlphaLumaKernel_param_6];
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r10, %r11, %r12;
	setp.lt.s32	%p1, %r1, %r5;
	setp.lt.s32	%p2, %r2, %r6;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB6_13;
	bra.uni 	BB6_1;

BB6_1:
	mad.lo.s32 	%r13, %r2, %r3, %r1;
	cvt.s64.s32	%rd1, %r13;
	setp.eq.s32	%p4, %r4, 0;
	@%p4 bra 	BB6_3;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v4.f32 	{%f30, %f31, %f32, %f33}, [%rd7];
	mov.f32 	%f51, %f33;
	mov.f32 	%f50, %f32;
	mov.f32 	%f49, %f31;
	mov.f32 	%f48, %f30;
	bra.uni 	BB6_4;

BB6_3:
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 3;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd10];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs1;
	cvt.f32.f16 	%f48, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs2;
	cvt.f32.f16 	%f49, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs3;
	cvt.f32.f16 	%f50, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs4;
	cvt.f32.f16 	%f51, %temp;
	}

BB6_4:
	@%p4 bra 	BB6_6;

	cvta.to.global.u64 	%rd11, %rd2;
	shl.b64 	%rd12, %rd1, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.v4.f32 	{%f34, %f35, %f36, %f37}, [%rd13];
	mov.f32 	%f55, %f37;
	mov.f32 	%f54, %f36;
	mov.f32 	%f53, %f35;
	mov.f32 	%f52, %f34;
	bra.uni 	BB6_7;

BB6_6:
	cvta.to.global.u64 	%rd14, %rd2;
	shl.b64 	%rd15, %rd1, 3;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [%rd16];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs9;
	cvt.f32.f16 	%f52, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs10;
	cvt.f32.f16 	%f53, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs11;
	cvt.f32.f16 	%f54, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs12;
	cvt.f32.f16 	%f55, %temp;
	}

BB6_7:
	ld.const.f32 	%f39, [kRGB32f_To_601YPbPr];
	ld.const.f32 	%f40, [kRGB32f_To_601YPbPr+4];
	mul.ftz.f32 	%f41, %f53, %f40;
	fma.rn.ftz.f32 	%f42, %f39, %f54, %f41;
	ld.const.f32 	%f43, [kRGB32f_To_601YPbPr+8];
	fma.rn.ftz.f32 	%f44, %f52, %f43, %f42;
	mul.ftz.f32 	%f45, %f51, %f44;
	mul.ftz.f32 	%f25, %f55, %f45;
	setp.gt.ftz.f32	%p6, %f25, 0f3F800000;
	mov.f32 	%f38, 0f3F800000;
	mov.f32 	%f56, %f38;
	@%p6 bra 	BB6_10;

	ld.const.f32 	%f46, [kMinAlphaValue];
	setp.geu.ftz.f32	%p7, %f25, %f46;
	mov.f32 	%f56, %f25;
	@%p7 bra 	BB6_10;

	mov.f32 	%f56, 0f00000000;

BB6_10:
	@%p4 bra 	BB6_12;

	cvta.to.global.u64 	%rd17, %rd4;
	shl.b64 	%rd18, %rd1, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.v4.f32 	[%rd19], {%f48, %f49, %f50, %f56};
	bra.uni 	BB6_13;

BB6_12:
	cvta.to.global.u64 	%rd20, %rd4;
	shl.b64 	%rd21, %rd1, 3;
	add.s64 	%rd22, %rd20, %rd21;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f56;
	mov.b16 	%rs17, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f50;
	mov.b16 	%rs18, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f49;
	mov.b16 	%rs19, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f48;
	mov.b16 	%rs20, %temp;
}
	st.global.v4.u16 	[%rd22], {%rs20, %rs19, %rs18, %rs17};

BB6_13:
	ret;
}

	// .globl	GlowMultNotAlphaKernel
.visible .entry GlowMultNotAlphaKernel(
	.param .u64 GlowMultNotAlphaKernel_param_0,
	.param .u64 GlowMultNotAlphaKernel_param_1,
	.param .u64 GlowMultNotAlphaKernel_param_2,
	.param .u32 GlowMultNotAlphaKernel_param_3,
	.param .u32 GlowMultNotAlphaKernel_param_4,
	.param .u32 GlowMultNotAlphaKernel_param_5,
	.param .u32 GlowMultNotAlphaKernel_param_6,
	.param .u32 GlowMultNotAlphaKernel_param_7
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<21>;
	.reg .f32 	%f<42>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd2, [GlowMultNotAlphaKernel_param_0];
	ld.param.u64 	%rd3, [GlowMultNotAlphaKernel_param_1];
	ld.param.u64 	%rd4, [GlowMultNotAlphaKernel_param_2];
	ld.param.u32 	%r3, [GlowMultNotAlphaKernel_param_3];
	ld.param.u32 	%r4, [GlowMultNotAlphaKernel_param_4];
	ld.param.u32 	%r5, [GlowMultNotAlphaKernel_param_5];
	ld.param.u32 	%r6, [GlowMultNotAlphaKernel_param_6];
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r10, %r11, %r12;
	setp.lt.s32	%p1, %r1, %r5;
	setp.lt.s32	%p2, %r2, %r6;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB7_10;
	bra.uni 	BB7_1;

BB7_1:
	mad.lo.s32 	%r13, %r2, %r3, %r1;
	cvt.s64.s32	%rd1, %r13;
	setp.eq.s32	%p4, %r4, 0;
	@%p4 bra 	BB7_3;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v4.f32 	{%f29, %f30, %f31, %f32}, [%rd7];
	mov.f32 	%f40, %f32;
	mov.f32 	%f39, %f31;
	mov.f32 	%f38, %f30;
	mov.f32 	%f37, %f29;
	bra.uni 	BB7_4;

BB7_3:
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 3;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd10];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs1;
	cvt.f32.f16 	%f37, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs2;
	cvt.f32.f16 	%f38, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs3;
	cvt.f32.f16 	%f39, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs4;
	cvt.f32.f16 	%f40, %temp;
	}

BB7_4:
	@%p4 bra 	BB7_6;

	cvta.to.global.u64 	%rd11, %rd2;
	shl.b64 	%rd12, %rd1, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.v4.f32 	{%f33, %f34, %f35, %f36}, [%rd13];
	mov.f32 	%f41, %f36;
	mov.f32 	%f15, %f35;
	mov.f32 	%f14, %f34;
	mov.f32 	%f13, %f33;
	bra.uni 	BB7_7;

BB7_6:
	cvta.to.global.u64 	%rd14, %rd2;
	shl.b64 	%rd15, %rd1, 3;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [%rd16];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs12;
	cvt.f32.f16 	%f41, %temp;
	}

BB7_7:
	cvt.ftz.f64.f32	%fd1, %f41;
	mov.f64 	%fd2, 0d3FF0000000000000;
	sub.f64 	%fd3, %fd2, %fd1;
	cvt.ftz.f64.f32	%fd4, %f40;
	mul.f64 	%fd5, %fd4, %fd3;
	cvt.rn.ftz.f32.f64	%f28, %fd5;
	@%p4 bra 	BB7_9;

	cvta.to.global.u64 	%rd17, %rd4;
	shl.b64 	%rd18, %rd1, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.v4.f32 	[%rd19], {%f37, %f38, %f39, %f28};
	bra.uni 	BB7_10;

BB7_9:
	cvta.to.global.u64 	%rd20, %rd4;
	shl.b64 	%rd21, %rd1, 3;
	add.s64 	%rd22, %rd20, %rd21;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f28;
	mov.b16 	%rs17, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f39;
	mov.b16 	%rs18, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f38;
	mov.b16 	%rs19, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f37;
	mov.b16 	%rs20, %temp;
}
	st.global.v4.u16 	[%rd22], {%rs20, %rs19, %rs18, %rs17};

BB7_10:
	ret;
}

	// .globl	GlowMultNotAlphaLumaKernel
.visible .entry GlowMultNotAlphaLumaKernel(
	.param .u64 GlowMultNotAlphaLumaKernel_param_0,
	.param .u64 GlowMultNotAlphaLumaKernel_param_1,
	.param .u64 GlowMultNotAlphaLumaKernel_param_2,
	.param .u32 GlowMultNotAlphaLumaKernel_param_3,
	.param .u32 GlowMultNotAlphaLumaKernel_param_4,
	.param .u32 GlowMultNotAlphaLumaKernel_param_5,
	.param .u32 GlowMultNotAlphaLumaKernel_param_6,
	.param .u32 GlowMultNotAlphaLumaKernel_param_7
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<21>;
	.reg .f32 	%f<57>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd2, [GlowMultNotAlphaLumaKernel_param_0];
	ld.param.u64 	%rd3, [GlowMultNotAlphaLumaKernel_param_1];
	ld.param.u64 	%rd4, [GlowMultNotAlphaLumaKernel_param_2];
	ld.param.u32 	%r3, [GlowMultNotAlphaLumaKernel_param_3];
	ld.param.u32 	%r4, [GlowMultNotAlphaLumaKernel_param_4];
	ld.param.u32 	%r5, [GlowMultNotAlphaLumaKernel_param_5];
	ld.param.u32 	%r6, [GlowMultNotAlphaLumaKernel_param_6];
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r10, %r11, %r12;
	setp.lt.s32	%p1, %r1, %r5;
	setp.lt.s32	%p2, %r2, %r6;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB8_13;
	bra.uni 	BB8_1;

BB8_1:
	mad.lo.s32 	%r13, %r2, %r3, %r1;
	cvt.s64.s32	%rd1, %r13;
	setp.eq.s32	%p4, %r4, 0;
	@%p4 bra 	BB8_3;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v4.f32 	{%f30, %f31, %f32, %f33}, [%rd7];
	mov.f32 	%f51, %f33;
	mov.f32 	%f50, %f32;
	mov.f32 	%f49, %f31;
	mov.f32 	%f48, %f30;
	bra.uni 	BB8_4;

BB8_3:
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 3;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd10];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs1;
	cvt.f32.f16 	%f48, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs2;
	cvt.f32.f16 	%f49, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs3;
	cvt.f32.f16 	%f50, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs4;
	cvt.f32.f16 	%f51, %temp;
	}

BB8_4:
	@%p4 bra 	BB8_6;

	cvta.to.global.u64 	%rd11, %rd2;
	shl.b64 	%rd12, %rd1, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.v4.f32 	{%f34, %f35, %f36, %f37}, [%rd13];
	mov.f32 	%f55, %f37;
	mov.f32 	%f54, %f36;
	mov.f32 	%f53, %f35;
	mov.f32 	%f52, %f34;
	bra.uni 	BB8_7;

BB8_6:
	cvta.to.global.u64 	%rd14, %rd2;
	shl.b64 	%rd15, %rd1, 3;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [%rd16];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs9;
	cvt.f32.f16 	%f52, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs10;
	cvt.f32.f16 	%f53, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs11;
	cvt.f32.f16 	%f54, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs12;
	cvt.f32.f16 	%f55, %temp;
	}

BB8_7:
	ld.const.f32 	%f39, [kRGB32f_To_601YPbPr];
	ld.const.f32 	%f40, [kRGB32f_To_601YPbPr+4];
	mul.ftz.f32 	%f41, %f53, %f40;
	fma.rn.ftz.f32 	%f42, %f39, %f54, %f41;
	ld.const.f32 	%f43, [kRGB32f_To_601YPbPr+8];
	fma.rn.ftz.f32 	%f44, %f52, %f43, %f42;
	mul.ftz.f32 	%f45, %f55, %f44;
	cvt.ftz.f64.f32	%fd1, %f45;
	mov.f64 	%fd2, 0d3FF0000000000000;
	sub.f64 	%fd3, %fd2, %fd1;
	cvt.ftz.f64.f32	%fd4, %f51;
	mul.f64 	%fd5, %fd4, %fd3;
	cvt.rn.ftz.f32.f64	%f25, %fd5;
	setp.gt.ftz.f32	%p6, %f25, 0f3F800000;
	mov.f32 	%f38, 0f3F800000;
	mov.f32 	%f56, %f38;
	@%p6 bra 	BB8_10;

	ld.const.f32 	%f46, [kMinAlphaValue];
	setp.geu.ftz.f32	%p7, %f25, %f46;
	mov.f32 	%f56, %f25;
	@%p7 bra 	BB8_10;

	mov.f32 	%f56, 0f00000000;

BB8_10:
	@%p4 bra 	BB8_12;

	cvta.to.global.u64 	%rd17, %rd4;
	shl.b64 	%rd18, %rd1, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.v4.f32 	[%rd19], {%f48, %f49, %f50, %f56};
	bra.uni 	BB8_13;

BB8_12:
	cvta.to.global.u64 	%rd20, %rd4;
	shl.b64 	%rd21, %rd1, 3;
	add.s64 	%rd22, %rd20, %rd21;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f56;
	mov.b16 	%rs17, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f50;
	mov.b16 	%rs18, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f49;
	mov.b16 	%rs19, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f48;
	mov.b16 	%rs20, %temp;
}
	st.global.v4.u16 	[%rd22], {%rs20, %rs19, %rs18, %rs17};

BB8_13:
	ret;
}

	// .globl	GlowAdditivePremulKernel
.visible .entry GlowAdditivePremulKernel(
	.param .u64 GlowAdditivePremulKernel_param_0,
	.param .u64 GlowAdditivePremulKernel_param_1,
	.param .u64 GlowAdditivePremulKernel_param_2,
	.param .u32 GlowAdditivePremulKernel_param_3,
	.param .u32 GlowAdditivePremulKernel_param_4,
	.param .u32 GlowAdditivePremulKernel_param_5,
	.param .u32 GlowAdditivePremulKernel_param_6,
	.param .u32 GlowAdditivePremulKernel_param_7
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<21>;
	.reg .f32 	%f<72>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd2, [GlowAdditivePremulKernel_param_0];
	ld.param.u64 	%rd3, [GlowAdditivePremulKernel_param_1];
	ld.param.u64 	%rd4, [GlowAdditivePremulKernel_param_2];
	ld.param.u32 	%r3, [GlowAdditivePremulKernel_param_3];
	ld.param.u32 	%r4, [GlowAdditivePremulKernel_param_4];
	ld.param.u32 	%r6, [GlowAdditivePremulKernel_param_5];
	ld.param.u32 	%r7, [GlowAdditivePremulKernel_param_6];
	ld.param.u32 	%r5, [GlowAdditivePremulKernel_param_7];
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r8, %r9, %r10;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r11, %r12, %r13;
	setp.lt.s32	%p1, %r1, %r6;
	setp.lt.s32	%p2, %r2, %r7;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB9_14;
	bra.uni 	BB9_1;

BB9_1:
	mad.lo.s32 	%r14, %r2, %r3, %r1;
	cvt.s64.s32	%rd1, %r14;
	setp.eq.s32	%p4, %r4, 0;
	@%p4 bra 	BB9_3;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v4.f32 	{%f44, %f45, %f46, %f47}, [%rd7];
	mov.f32 	%f63, %f47;
	mov.f32 	%f62, %f46;
	mov.f32 	%f61, %f45;
	mov.f32 	%f60, %f44;
	bra.uni 	BB9_4;

BB9_3:
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 3;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd10];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs1;
	cvt.f32.f16 	%f60, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs2;
	cvt.f32.f16 	%f61, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs3;
	cvt.f32.f16 	%f62, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs4;
	cvt.f32.f16 	%f63, %temp;
	}

BB9_4:
	@%p4 bra 	BB9_6;

	cvta.to.global.u64 	%rd11, %rd2;
	shl.b64 	%rd12, %rd1, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.v4.f32 	{%f48, %f49, %f50, %f51}, [%rd13];
	mov.f32 	%f67, %f51;
	mov.f32 	%f66, %f50;
	mov.f32 	%f65, %f49;
	mov.f32 	%f64, %f48;
	bra.uni 	BB9_7;

BB9_6:
	cvta.to.global.u64 	%rd14, %rd2;
	shl.b64 	%rd15, %rd1, 3;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [%rd16];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs9;
	cvt.f32.f16 	%f64, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs10;
	cvt.f32.f16 	%f65, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs11;
	cvt.f32.f16 	%f66, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs12;
	cvt.f32.f16 	%f67, %temp;
	}

BB9_7:
	mov.f32 	%f71, %f60;
	mov.f32 	%f70, %f61;
	mov.f32 	%f69, %f62;
	mov.f32 	%f28, %f63;
	mov.f32 	%f52, 0f3F800000;
	sub.ftz.f32 	%f53, %f52, %f67;
	mul.ftz.f32 	%f29, %f63, %f53;
	sub.ftz.f32 	%f54, %f52, %f63;
	mul.ftz.f32 	%f55, %f54, %f53;
	sub.ftz.f32 	%f30, %f52, %f55;
	ld.const.f32 	%f56, [kMinAlphaValue];
	setp.leu.ftz.f32	%p6, %f30, %f56;
	mov.f32 	%f68, %f28;
	@%p6 bra 	BB9_9;

	fma.rn.ftz.f32 	%f57, %f62, %f29, %f66;
	div.approx.ftz.f32 	%f69, %f57, %f30;
	fma.rn.ftz.f32 	%f58, %f61, %f29, %f65;
	div.approx.ftz.f32 	%f70, %f58, %f30;
	fma.rn.ftz.f32 	%f59, %f60, %f29, %f64;
	div.approx.ftz.f32 	%f71, %f59, %f30;
	mov.f32 	%f68, %f30;

BB9_9:
	setp.eq.s32	%p7, %r5, 0;
	@%p7 bra 	BB9_11;

	cvt.ftz.sat.f32.f32	%f69, %f69;
	cvt.ftz.sat.f32.f32	%f70, %f70;
	cvt.ftz.sat.f32.f32	%f71, %f71;

BB9_11:
	@%p4 bra 	BB9_13;

	cvta.to.global.u64 	%rd17, %rd4;
	shl.b64 	%rd18, %rd1, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.v4.f32 	[%rd19], {%f71, %f70, %f69, %f68};
	bra.uni 	BB9_14;

BB9_13:
	cvta.to.global.u64 	%rd20, %rd4;
	shl.b64 	%rd21, %rd1, 3;
	add.s64 	%rd22, %rd20, %rd21;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f68;
	mov.b16 	%rs17, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f69;
	mov.b16 	%rs18, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f70;
	mov.b16 	%rs19, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f71;
	mov.b16 	%rs20, %temp;
}
	st.global.v4.u16 	[%rd22], {%rs20, %rs19, %rs18, %rs17};

BB9_14:
	ret;
}

	// .globl	GlowAlphaAddKernel
.visible .entry GlowAlphaAddKernel(
	.param .u64 GlowAlphaAddKernel_param_0,
	.param .u64 GlowAlphaAddKernel_param_1,
	.param .u64 GlowAlphaAddKernel_param_2,
	.param .u32 GlowAlphaAddKernel_param_3,
	.param .u32 GlowAlphaAddKernel_param_4,
	.param .u32 GlowAlphaAddKernel_param_5,
	.param .u32 GlowAlphaAddKernel_param_6,
	.param .u32 GlowAlphaAddKernel_param_7
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<21>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd2, [GlowAlphaAddKernel_param_0];
	ld.param.u64 	%rd3, [GlowAlphaAddKernel_param_1];
	ld.param.u64 	%rd4, [GlowAlphaAddKernel_param_2];
	ld.param.u32 	%r3, [GlowAlphaAddKernel_param_3];
	ld.param.u32 	%r4, [GlowAlphaAddKernel_param_4];
	ld.param.u32 	%r5, [GlowAlphaAddKernel_param_5];
	ld.param.u32 	%r6, [GlowAlphaAddKernel_param_6];
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r10, %r11, %r12;
	setp.lt.s32	%p1, %r1, %r5;
	setp.lt.s32	%p2, %r2, %r6;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB10_15;
	bra.uni 	BB10_1;

BB10_1:
	mad.lo.s32 	%r13, %r2, %r3, %r1;
	cvt.s64.s32	%rd1, %r13;
	setp.eq.s32	%p4, %r4, 0;
	@%p4 bra 	BB10_3;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.v4.f32 	{%f44, %f45, %f46, %f47}, [%rd7];
	mov.f32 	%f67, %f47;
	mov.f32 	%f66, %f46;
	mov.f32 	%f65, %f45;
	mov.f32 	%f64, %f44;
	bra.uni 	BB10_4;

BB10_3:
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 3;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd10];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs1;
	cvt.f32.f16 	%f64, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs2;
	cvt.f32.f16 	%f65, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs3;
	cvt.f32.f16 	%f66, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs4;
	cvt.f32.f16 	%f67, %temp;
	}

BB10_4:
	@%p4 bra 	BB10_6;

	cvta.to.global.u64 	%rd11, %rd2;
	shl.b64 	%rd12, %rd1, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.v4.f32 	{%f48, %f49, %f50, %f51}, [%rd13];
	mov.f32 	%f71, %f51;
	mov.f32 	%f70, %f50;
	mov.f32 	%f69, %f49;
	mov.f32 	%f68, %f48;
	bra.uni 	BB10_7;

BB10_6:
	cvta.to.global.u64 	%rd14, %rd2;
	shl.b64 	%rd15, %rd1, 3;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [%rd16];
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs9;
	cvt.f32.f16 	%f68, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs10;
	cvt.f32.f16 	%f69, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs11;
	cvt.f32.f16 	%f70, %temp;
	}
	{
	.reg .b16 %temp;
	mov.b16 	%temp, %rs12;
	cvt.f32.f16 	%f71, %temp;
	}

BB10_7:
	mov.f32 	%f25, %f67;
	mov.f32 	%f75, %f68;
	mov.f32 	%f74, %f69;
	mov.f32 	%f73, %f70;
	mov.f32 	%f29, %f71;
	add.ftz.f32 	%f30, %f67, %f71;
	setp.eq.ftz.f32	%p6, %f71, 0f3F800000;
	mov.f32 	%f72, %f29;
	@%p6 bra 	BB10_12;

	mov.f32 	%f75, %f64;
	mov.f32 	%f74, %f65;
	mov.f32 	%f73, %f66;
	setp.ltu.ftz.f32	%p7, %f30, 0f3F800000;
	@%p7 bra 	BB10_10;
	bra.uni 	BB10_9;

BB10_10:
	ld.const.f32 	%f57, [kMinAlphaValue];
	setp.leu.ftz.f32	%p8, %f30, %f57;
	mov.f32 	%f72, %f25;
	@%p8 bra 	BB10_12;

	mul.ftz.f32 	%f58, %f70, %f71;
	fma.rn.ftz.f32 	%f59, %f66, %f67, %f58;
	div.approx.ftz.f32 	%f73, %f59, %f30;
	mul.ftz.f32 	%f60, %f69, %f71;
	fma.rn.ftz.f32 	%f61, %f65, %f67, %f60;
	div.approx.ftz.f32 	%f74, %f61, %f30;
	mul.ftz.f32 	%f62, %f68, %f71;
	fma.rn.ftz.f32 	%f63, %f64, %f67, %f62;
	div.approx.ftz.f32 	%f75, %f63, %f30;
	mov.f32 	%f72, %f30;
	bra.uni 	BB10_12;

BB10_9:
	mov.f32 	%f72, 0f3F800000;
	sub.ftz.f32 	%f53, %f72, %f71;
	mul.ftz.f32 	%f54, %f66, %f53;
	fma.rn.ftz.f32 	%f73, %f70, %f71, %f54;
	mul.ftz.f32 	%f55, %f65, %f53;
	fma.rn.ftz.f32 	%f74, %f69, %f71, %f55;
	mul.ftz.f32 	%f56, %f64, %f53;
	fma.rn.ftz.f32 	%f75, %f68, %f71, %f56;

BB10_12:
	@%p4 bra 	BB10_14;

	cvta.to.global.u64 	%rd17, %rd4;
	shl.b64 	%rd18, %rd1, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.v4.f32 	[%rd19], {%f75, %f74, %f73, %f72};
	bra.uni 	BB10_15;

BB10_14:
	cvta.to.global.u64 	%rd20, %rd4;
	shl.b64 	%rd21, %rd1, 3;
	add.s64 	%rd22, %rd20, %rd21;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f72;
	mov.b16 	%rs17, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f73;
	mov.b16 	%rs18, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f74;
	mov.b16 	%rs19, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f75;
	mov.b16 	%rs20, %temp;
}
	st.global.v4.u16 	[%rd22], {%rs20, %rs19, %rs18, %rs17};

BB10_15:
	ret;
}


