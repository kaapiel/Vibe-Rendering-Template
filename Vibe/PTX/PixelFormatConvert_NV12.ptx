//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21112126
// Cuda compilation tools, release 8.0, V8.0.43
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_30
.address_size 64

	// .globl	PixelFormatConvert_NV12_FIELD_601_Kernel
.global .texref inTexture;
.const .align 4 .b8 kRGB32f_To_601YPbPr[36] = {135, 22, 153, 62, 162, 69, 22, 63, 213, 120, 233, 61, 33, 201, 44, 190, 111, 155, 169, 190, 0, 0, 0, 63, 0, 0, 0, 63, 70, 94, 214, 190, 232, 134, 166, 189};
.const .align 4 .b8 k601YPbPr_To_RGB32f[36] = {0, 0, 128, 63, 0, 0, 0, 0, 188, 116, 179, 63, 0, 0, 128, 63, 152, 50, 176, 190, 158, 209, 54, 191, 0, 0, 128, 63, 229, 208, 226, 63, 0, 0, 0, 0};
.const .align 4 .b8 kRGB32f_To_601YCbCr[36] = {70, 246, 130, 66, 145, 141, 0, 67, 94, 186, 199, 65, 33, 48, 23, 194, 240, 103, 148, 194, 0, 0, 224, 66, 0, 0, 224, 66, 111, 146, 187, 194, 70, 182, 145, 193};
.const .align 4 .b8 k601YCbCr_To_RGB32f[36] = {37, 160, 149, 59, 0, 0, 0, 0, 182, 23, 205, 59, 37, 160, 149, 59, 40, 15, 201, 186, 156, 239, 80, 187, 37, 160, 149, 59, 236, 155, 1, 60, 0, 0, 0, 0};
.const .align 4 .b8 kRGB8u_To_601YCbCr[36] = {219, 121, 131, 62, 152, 14, 1, 63, 18, 131, 200, 61, 174, 199, 23, 190, 238, 252, 148, 190, 197, 224, 224, 62, 197, 224, 224, 62, 217, 78, 188, 190, 174, 71, 146, 189};
.const .align 4 .b8 k601YCbCr_To_RGB8u[36] = {127, 10, 149, 63, 0, 0, 0, 0, 160, 74, 204, 63, 127, 10, 149, 63, 254, 148, 200, 190, 184, 30, 80, 191, 127, 10, 149, 63, 78, 26, 1, 64, 0, 0, 0, 0};
.const .align 4 .b8 kRGB8u_To_601YCbCrFullRange[36] = {135, 22, 153, 62, 162, 69, 22, 63, 213, 120, 233, 61, 166, 27, 44, 190, 39, 241, 168, 190, 250, 254, 254, 62, 250, 254, 254, 62, 43, 135, 213, 190, 59, 223, 165, 189};
.const .align 4 .b8 k601YCbCrFullRange_To_RGB8u[36] = {0, 0, 128, 63, 0, 0, 0, 0, 72, 193, 178, 63, 0, 0, 128, 63, 143, 130, 175, 190, 225, 26, 54, 191, 0, 0, 128, 63, 20, 238, 225, 63, 0, 0, 0, 0};
.const .align 4 .b8 kRGB32f_To_601YCbCrFullRange[36] = {113, 125, 152, 66, 92, 175, 21, 67, 92, 143, 232, 65, 158, 111, 43, 194, 49, 72, 168, 194, 0, 0, 254, 66, 0, 0, 254, 66, 170, 177, 212, 194, 88, 57, 165, 193};
.const .align 4 .b8 k601YCbCrFullRange_To_RGB32f[36] = {129, 128, 128, 59, 0, 0, 0, 0, 188, 116, 179, 59, 129, 128, 128, 59, 194, 50, 176, 186, 179, 209, 54, 187, 129, 128, 128, 59, 229, 208, 226, 59, 0, 0, 0, 0};
.const .align 4 .b8 kRGB32f_To_709YPbPr[36] = {208, 179, 89, 62, 89, 23, 55, 63, 152, 221, 147, 61, 186, 164, 234, 189, 210, 86, 197, 190, 0, 0, 0, 63, 0, 0, 0, 63, 190, 134, 232, 190, 16, 202, 59, 189};
.const .align 4 .b8 k709YPbPr_To_RGB32f[36] = {0, 0, 128, 63, 0, 0, 0, 0, 12, 147, 201, 63, 0, 0, 128, 63, 221, 209, 63, 190, 243, 173, 239, 190, 0, 0, 128, 63, 77, 132, 237, 63, 0, 0, 0, 0};
.const .align 4 .b8 kRGB32f_To_709YCbCr[36] = {106, 60, 58, 66, 6, 161, 28, 67, 244, 253, 124, 65, 223, 79, 205, 193, 8, 172, 172, 194, 0, 0, 224, 66, 0, 0, 224, 66, 195, 117, 203, 194, 236, 81, 36, 193};
.const .align 4 .b8 k709YCbCr_To_RGB32f[36] = {37, 160, 149, 59, 0, 0, 0, 0, 239, 94, 230, 59, 37, 160, 149, 59, 33, 57, 91, 186, 178, 245, 8, 187, 37, 160, 149, 59, 82, 185, 7, 60, 0, 0, 0, 0};
.const .align 4 .b8 k709YCbCrFullRange_To_RGB32f[36] = {131, 128, 128, 59, 0, 0, 0, 0, 28, 147, 201, 59, 131, 128, 128, 59, 61, 210, 63, 186, 248, 173, 239, 186, 131, 128, 128, 59, 82, 132, 237, 59, 0, 0, 0, 0};
.const .align 4 .b8 kRGB8u_To_709YCbCr[36] = {207, 247, 58, 62, 53, 62, 29, 63, 231, 251, 125, 61, 184, 30, 206, 189, 23, 89, 173, 190, 197, 224, 224, 62, 197, 224, 224, 62, 12, 66, 204, 190, 195, 245, 36, 189};
.const .align 4 .b8 k709YCbCr_To_RGB8u[36] = {127, 10, 149, 63, 0, 0, 0, 0, 147, 120, 229, 63, 127, 10, 149, 63, 53, 94, 90, 190, 205, 108, 8, 191, 127, 10, 149, 63, 154, 49, 7, 64, 0, 0, 0, 0};
.const .align 4 .b8 k709YCbCr_To_601YCbCr[36] = {0, 0, 128, 63, 23, 100, 203, 61, 1, 77, 68, 62, 0, 0, 0, 0, 18, 103, 125, 63, 10, 158, 226, 189, 0, 0, 0, 0, 61, 98, 148, 189, 249, 191, 123, 63};
.const .align 4 .b8 k601YCbCr_To_709YCbCr[36] = {0, 0, 128, 63, 122, 165, 236, 189, 179, 237, 84, 190, 0, 0, 0, 0, 204, 98, 130, 63, 216, 188, 234, 61, 0, 0, 0, 0, 74, 179, 153, 61, 234, 61, 131, 63};
.const .align 4 .b8 kYCbCrOffset[12] = {0, 0, 128, 65, 0, 0, 0, 67, 0, 0, 0, 67};
.const .align 4 .b8 kYCbCrFullRangeOffset[12] = {0, 0, 0, 0, 0, 0, 0, 67, 0, 0, 0, 67};

.visible .entry PixelFormatConvert_NV12_FIELD_601_Kernel(
	.param .u64 PixelFormatConvert_NV12_FIELD_601_Kernel_param_0,
	.param .u64 PixelFormatConvert_NV12_FIELD_601_Kernel_param_1,
	.param .u32 PixelFormatConvert_NV12_FIELD_601_Kernel_param_2,
	.param .u32 PixelFormatConvert_NV12_FIELD_601_Kernel_param_3,
	.param .u32 PixelFormatConvert_NV12_FIELD_601_Kernel_param_4,
	.param .u32 PixelFormatConvert_NV12_FIELD_601_Kernel_param_5
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<17>;
	.reg .f32 	%f<126>;
	.reg .b32 	%r<95>;
	.reg .b64 	%rd<30>;


	ld.param.u64 	%rd5, [PixelFormatConvert_NV12_FIELD_601_Kernel_param_1];
	ld.param.u32 	%r20, [PixelFormatConvert_NV12_FIELD_601_Kernel_param_2];
	ld.param.u32 	%r21, [PixelFormatConvert_NV12_FIELD_601_Kernel_param_3];
	ld.param.u32 	%r22, [PixelFormatConvert_NV12_FIELD_601_Kernel_param_4];
	ld.param.u32 	%r23, [PixelFormatConvert_NV12_FIELD_601_Kernel_param_5];
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %ntid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r1, %r25, %r24, %r26;
	mov.u32 	%r27, %ntid.y;
	mov.u32 	%r28, %ctaid.y;
	mov.u32 	%r29, %tid.y;
	mad.lo.s32 	%r2, %r27, %r28, %r29;
	shl.b32 	%r30, %r1, 1;
	setp.ge.u32	%p1, %r30, %r22;
	setp.ge.u32	%p2, %r2, %r23;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB0_13;

	cvt.rn.f32.u32	%f1, %r2;
	add.ftz.f32 	%f30, %f1, 0fBE000000;
	mov.f32 	%f31, 0f00000000;
	max.ftz.f32 	%f32, %f30, %f31;
	cvt.rn.f32.u32	%f2, %r23;
	min.ftz.f32 	%f33, %f32, %f2;
	fma.rn.ftz.f32 	%f34, %f33, 0f3F000000, %f2;
	add.ftz.f32 	%f35, %f34, 0f3F000000;
	cvt.rn.f32.u32	%f3, %r1;
	add.ftz.f32 	%f36, %f3, 0f3F000000;
	cvt.rn.f32.u32	%f4, %r22;
	min.ftz.f32 	%f5, %f36, %f4;
	add.ftz.f32 	%f37, %f5, 0f00000000;
	add.ftz.f32 	%f38, %f37, 0f3F000000;
	tex.2d.v4.u32.f32	{%r31, %r32, %r33, %r34}, [inTexture, {%f38, %f35}];
	mov.b32 	 %f39, %r31;
	mov.b32 	 %f40, %r32;
	tex.2d.v4.u32.f32	{%r3, %r4, %r5, %r6}, [inTexture, {%f36, %f35}];
	add.ftz.f32 	%f41, %f1, 0f3F000000;
	tex.2d.v4.u32.f32	{%r7, %r8, %r9, %r10}, [inTexture, {%f36, %f41}];
	mov.b32 	 %f42, %r8;
	mul.ftz.f32 	%f43, %f40, 0f437F0000;
	mul.ftz.f32 	%f44, %f39, 0f437F0000;
	mul.ftz.f32 	%f45, %f42, 0f437F0000;
	ld.const.f32 	%f46, [kYCbCrOffset];
	mov.f32 	%f47, 0f437F0000;
	div.approx.ftz.f32 	%f48, %f47, %f47;
	mul.ftz.f32 	%f6, %f48, %f46;
	sub.ftz.f32 	%f49, %f45, %f6;
	ld.const.f32 	%f50, [kYCbCrOffset+4];
	mul.ftz.f32 	%f7, %f48, %f50;
	sub.ftz.f32 	%f51, %f44, %f7;
	ld.const.f32 	%f52, [kYCbCrOffset+8];
	mul.ftz.f32 	%f8, %f48, %f52;
	sub.ftz.f32 	%f53, %f43, %f8;
	ld.const.f32 	%f9, [k601YCbCr_To_RGB32f];
	ld.const.f32 	%f10, [k601YCbCr_To_RGB32f+4];
	mul.ftz.f32 	%f54, %f51, %f10;
	fma.rn.ftz.f32 	%f55, %f49, %f9, %f54;
	ld.const.f32 	%f11, [k601YCbCr_To_RGB32f+8];
	fma.rn.ftz.f32 	%f12, %f53, %f11, %f55;
	ld.const.f32 	%f13, [k601YCbCr_To_RGB32f+12];
	ld.const.f32 	%f14, [k601YCbCr_To_RGB32f+16];
	mul.ftz.f32 	%f56, %f51, %f14;
	fma.rn.ftz.f32 	%f57, %f49, %f13, %f56;
	ld.const.f32 	%f15, [k601YCbCr_To_RGB32f+20];
	fma.rn.ftz.f32 	%f16, %f53, %f15, %f57;
	ld.const.f32 	%f17, [k601YCbCr_To_RGB32f+24];
	ld.const.f32 	%f18, [k601YCbCr_To_RGB32f+28];
	mul.ftz.f32 	%f58, %f51, %f18;
	fma.rn.ftz.f32 	%f59, %f49, %f17, %f58;
	ld.const.f32 	%f19, [k601YCbCr_To_RGB32f+32];
	fma.rn.ftz.f32 	%f20, %f53, %f19, %f59;
	setp.eq.s32	%p4, %r21, 0;
	@%p4 bra 	BB0_3;

	cvta.to.global.u64 	%rd6, %rd5;
	mul.lo.s32 	%r39, %r2, %r20;
	shl.b32 	%r40, %r39, 1;
	add.s32 	%r46, %r30, %r40;
	add.s32 	%r47, %r46, 1;
	mul.wide.s32 	%rd7, %r47, 16;
	add.s64 	%rd8, %rd6, %rd7;
	mov.f32 	%f60, 0f3F800000;
	st.global.v4.f32 	[%rd8], {%f20, %f16, %f12, %f60};
	bra.uni 	BB0_4;

BB0_3:
	cvta.to.global.u64 	%rd9, %rd5;
	mul.lo.s32 	%r52, %r2, %r20;
	shl.b32 	%r53, %r52, 1;
	add.s32 	%r59, %r30, %r53;
	add.s32 	%r60, %r59, 1;
	mul.wide.s32 	%rd10, %r60, 8;
	add.s64 	%rd11, %rd9, %rd10;
	mov.f32 	%f61, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f61;
	mov.b16 	%rs1, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f12;
	mov.b16 	%rs2, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f16;
	mov.b16 	%rs3, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f20;
	mov.b16 	%rs4, %temp;
}
	st.global.v4.u16 	[%rd11], {%rs4, %rs3, %rs2, %rs1};

BB0_4:
	mov.b32 	 %f62, %r3;
	mov.b32 	 %f63, %r4;
	mov.b32 	 %f64, %r7;
	mul.ftz.f32 	%f65, %f64, 0f437F0000;
	sub.ftz.f32 	%f66, %f65, %f6;
	mul.ftz.f32 	%f67, %f62, 0f437F0000;
	sub.ftz.f32 	%f68, %f67, %f7;
	mul.ftz.f32 	%f69, %f63, 0f437F0000;
	sub.ftz.f32 	%f70, %f69, %f8;
	mul.ftz.f32 	%f71, %f68, %f10;
	fma.rn.ftz.f32 	%f72, %f66, %f9, %f71;
	fma.rn.ftz.f32 	%f21, %f70, %f11, %f72;
	mul.ftz.f32 	%f73, %f68, %f14;
	fma.rn.ftz.f32 	%f74, %f66, %f13, %f73;
	fma.rn.ftz.f32 	%f22, %f70, %f15, %f74;
	mul.ftz.f32 	%f75, %f68, %f18;
	fma.rn.ftz.f32 	%f76, %f66, %f17, %f75;
	fma.rn.ftz.f32 	%f23, %f70, %f19, %f76;
	shl.b32 	%r70, %r2, 1;
	mad.lo.s32 	%r71, %r70, %r20, %r30;
	cvt.s64.s32	%rd2, %r71;
	@%p4 bra 	BB0_6;

	cvta.to.global.u64 	%rd12, %rd5;
	shl.b64 	%rd13, %rd2, 4;
	add.s64 	%rd14, %rd12, %rd13;
	mov.f32 	%f77, 0f3F800000;
	st.global.v4.f32 	[%rd14], {%f23, %f22, %f21, %f77};
	bra.uni 	BB0_7;

BB0_6:
	cvta.to.global.u64 	%rd15, %rd5;
	shl.b64 	%rd16, %rd2, 3;
	add.s64 	%rd17, %rd15, %rd16;
	mov.f32 	%f78, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f78;
	mov.b16 	%rs5, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f21;
	mov.b16 	%rs6, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f22;
	mov.b16 	%rs7, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f23;
	mov.b16 	%rs8, %temp;
}
	st.global.v4.u16 	[%rd17], {%rs8, %rs7, %rs6, %rs5};

BB0_7:
	add.ftz.f32 	%f79, %f1, 0f3E000000;
	max.ftz.f32 	%f81, %f79, %f31;
	min.ftz.f32 	%f82, %f81, %f2;
	fma.rn.ftz.f32 	%f83, %f82, 0f3F000000, %f2;
	add.ftz.f32 	%f84, %f83, 0f3F000000;
	mov.f32 	%f85, 0f40000000;
	div.approx.ftz.f32 	%f86, %f4, %f85;
	add.ftz.f32 	%f87, %f5, %f86;
	add.ftz.f32 	%f88, %f87, 0f3F000000;
	tex.2d.v4.u32.f32	{%r72, %r73, %r74, %r75}, [inTexture, {%f88, %f84}];
	mov.b32 	 %f89, %r72;
	mov.b32 	 %f90, %r73;
	add.ftz.f32 	%f91, %f3, %f86;
	add.ftz.f32 	%f92, %f91, 0f3F000000;
	tex.2d.v4.u32.f32	{%r11, %r12, %r13, %r14}, [inTexture, {%f92, %f84}];
	tex.2d.v4.u32.f32	{%r15, %r16, %r17, %r18}, [inTexture, {%f92, %f41}];
	add.s32 	%r81, %r70, 1;
	mov.b32 	 %f94, %r16;
	mul.ftz.f32 	%f95, %f90, 0f437F0000;
	mul.ftz.f32 	%f96, %f89, 0f437F0000;
	mul.ftz.f32 	%f97, %f94, 0f437F0000;
	sub.ftz.f32 	%f98, %f97, %f6;
	sub.ftz.f32 	%f99, %f96, %f7;
	sub.ftz.f32 	%f100, %f95, %f8;
	mul.ftz.f32 	%f101, %f99, %f10;
	fma.rn.ftz.f32 	%f102, %f98, %f9, %f101;
	fma.rn.ftz.f32 	%f24, %f100, %f11, %f102;
	mul.ftz.f32 	%f103, %f99, %f14;
	fma.rn.ftz.f32 	%f104, %f98, %f13, %f103;
	fma.rn.ftz.f32 	%f25, %f100, %f15, %f104;
	mul.ftz.f32 	%f105, %f99, %f18;
	fma.rn.ftz.f32 	%f106, %f98, %f17, %f105;
	fma.rn.ftz.f32 	%f26, %f100, %f19, %f106;
	mul.lo.s32 	%r19, %r81, %r20;
	add.s32 	%r87, %r30, %r19;
	add.s32 	%r88, %r87, 1;
	cvt.s64.s32	%rd3, %r88;
	@%p4 bra 	BB0_9;

	cvta.to.global.u64 	%rd18, %rd5;
	shl.b64 	%rd19, %rd3, 4;
	add.s64 	%rd20, %rd18, %rd19;
	mov.f32 	%f107, 0f3F800000;
	st.global.v4.f32 	[%rd20], {%f26, %f25, %f24, %f107};
	bra.uni 	BB0_10;

BB0_9:
	cvta.to.global.u64 	%rd21, %rd5;
	shl.b64 	%rd22, %rd3, 3;
	add.s64 	%rd23, %rd21, %rd22;
	mov.f32 	%f108, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f108;
	mov.b16 	%rs9, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f24;
	mov.b16 	%rs10, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f25;
	mov.b16 	%rs11, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f26;
	mov.b16 	%rs12, %temp;
}
	st.global.v4.u16 	[%rd23], {%rs12, %rs11, %rs10, %rs9};

BB0_10:
	mov.b32 	 %f109, %r11;
	mov.b32 	 %f110, %r12;
	mov.b32 	 %f111, %r15;
	mul.ftz.f32 	%f112, %f111, 0f437F0000;
	sub.ftz.f32 	%f113, %f112, %f6;
	mul.ftz.f32 	%f114, %f109, 0f437F0000;
	sub.ftz.f32 	%f115, %f114, %f7;
	mul.ftz.f32 	%f116, %f110, 0f437F0000;
	sub.ftz.f32 	%f117, %f116, %f8;
	mul.ftz.f32 	%f118, %f115, %f10;
	fma.rn.ftz.f32 	%f119, %f113, %f9, %f118;
	fma.rn.ftz.f32 	%f27, %f117, %f11, %f119;
	mul.ftz.f32 	%f120, %f115, %f14;
	fma.rn.ftz.f32 	%f121, %f113, %f13, %f120;
	fma.rn.ftz.f32 	%f28, %f117, %f15, %f121;
	mul.ftz.f32 	%f122, %f115, %f18;
	fma.rn.ftz.f32 	%f123, %f113, %f17, %f122;
	fma.rn.ftz.f32 	%f29, %f117, %f19, %f123;
	add.s32 	%r94, %r19, %r30;
	cvt.s64.s32	%rd4, %r94;
	@%p4 bra 	BB0_12;

	cvta.to.global.u64 	%rd24, %rd5;
	shl.b64 	%rd25, %rd4, 4;
	add.s64 	%rd26, %rd24, %rd25;
	mov.f32 	%f124, 0f3F800000;
	st.global.v4.f32 	[%rd26], {%f29, %f28, %f27, %f124};
	bra.uni 	BB0_13;

BB0_12:
	cvta.to.global.u64 	%rd27, %rd5;
	shl.b64 	%rd28, %rd4, 3;
	add.s64 	%rd29, %rd27, %rd28;
	mov.f32 	%f125, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f125;
	mov.b16 	%rs13, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f27;
	mov.b16 	%rs14, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f28;
	mov.b16 	%rs15, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f29;
	mov.b16 	%rs16, %temp;
}
	st.global.v4.u16 	[%rd29], {%rs16, %rs15, %rs14, %rs13};

BB0_13:
	ret;
}

	// .globl	PixelFormatConvert_NV12_FRAME_601_Kernel
.visible .entry PixelFormatConvert_NV12_FRAME_601_Kernel(
	.param .u64 PixelFormatConvert_NV12_FRAME_601_Kernel_param_0,
	.param .u64 PixelFormatConvert_NV12_FRAME_601_Kernel_param_1,
	.param .u32 PixelFormatConvert_NV12_FRAME_601_Kernel_param_2,
	.param .u32 PixelFormatConvert_NV12_FRAME_601_Kernel_param_3,
	.param .u32 PixelFormatConvert_NV12_FRAME_601_Kernel_param_4,
	.param .u32 PixelFormatConvert_NV12_FRAME_601_Kernel_param_5
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<73>;
	.reg .b32 	%r<58>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [PixelFormatConvert_NV12_FRAME_601_Kernel_param_1];
	ld.param.u32 	%r11, [PixelFormatConvert_NV12_FRAME_601_Kernel_param_2];
	ld.param.u32 	%r12, [PixelFormatConvert_NV12_FRAME_601_Kernel_param_3];
	ld.param.u32 	%r13, [PixelFormatConvert_NV12_FRAME_601_Kernel_param_4];
	ld.param.u32 	%r14, [PixelFormatConvert_NV12_FRAME_601_Kernel_param_5];
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r1, %r16, %r15, %r17;
	mov.u32 	%r18, %ntid.y;
	mov.u32 	%r19, %ctaid.y;
	mov.u32 	%r20, %tid.y;
	mad.lo.s32 	%r2, %r18, %r19, %r20;
	shl.b32 	%r21, %r1, 1;
	setp.ge.u32	%p1, %r21, %r13;
	setp.ge.u32	%p2, %r2, %r14;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB1_7;

	cvt.rn.f32.u32	%f19, %r2;
	add.ftz.f32 	%f20, %f19, 0fBE800000;
	mov.f32 	%f21, 0f00000000;
	max.ftz.f32 	%f22, %f20, %f21;
	cvt.rn.f32.u32	%f23, %r14;
	min.ftz.f32 	%f24, %f22, %f23;
	fma.rn.ftz.f32 	%f25, %f24, 0f3F000000, %f23;
	add.ftz.f32 	%f26, %f25, 0f3F000000;
	cvt.rn.f32.u32	%f27, %r1;
	add.ftz.f32 	%f28, %f27, 0f3F000000;
	cvt.rn.f32.u32	%f29, %r13;
	min.ftz.f32 	%f30, %f28, %f29;
	add.ftz.f32 	%f31, %f30, 0f00000000;
	add.ftz.f32 	%f32, %f31, 0f3F000000;
	tex.2d.v4.u32.f32	{%r22, %r23, %r24, %r25}, [inTexture, {%f32, %f26}];
	mov.b32 	 %f33, %r22;
	mov.b32 	 %f34, %r23;
	tex.2d.v4.u32.f32	{%r3, %r4, %r5, %r6}, [inTexture, {%f28, %f26}];
	add.ftz.f32 	%f35, %f19, 0f3F000000;
	tex.2d.v4.u32.f32	{%r7, %r8, %r9, %r10}, [inTexture, {%f28, %f35}];
	mov.b32 	 %f36, %r8;
	mul.ftz.f32 	%f37, %f34, 0f437F0000;
	mul.ftz.f32 	%f38, %f33, 0f437F0000;
	mul.ftz.f32 	%f39, %f36, 0f437F0000;
	ld.const.f32 	%f40, [kYCbCrOffset];
	mov.f32 	%f41, 0f437F0000;
	div.approx.ftz.f32 	%f42, %f41, %f41;
	mul.ftz.f32 	%f1, %f42, %f40;
	sub.ftz.f32 	%f43, %f39, %f1;
	ld.const.f32 	%f44, [kYCbCrOffset+4];
	mul.ftz.f32 	%f2, %f42, %f44;
	sub.ftz.f32 	%f45, %f38, %f2;
	ld.const.f32 	%f46, [kYCbCrOffset+8];
	mul.ftz.f32 	%f3, %f42, %f46;
	sub.ftz.f32 	%f47, %f37, %f3;
	ld.const.f32 	%f4, [k601YCbCr_To_RGB32f];
	ld.const.f32 	%f5, [k601YCbCr_To_RGB32f+4];
	mul.ftz.f32 	%f48, %f45, %f5;
	fma.rn.ftz.f32 	%f49, %f43, %f4, %f48;
	ld.const.f32 	%f6, [k601YCbCr_To_RGB32f+8];
	fma.rn.ftz.f32 	%f7, %f47, %f6, %f49;
	ld.const.f32 	%f8, [k601YCbCr_To_RGB32f+12];
	ld.const.f32 	%f9, [k601YCbCr_To_RGB32f+16];
	mul.ftz.f32 	%f50, %f45, %f9;
	fma.rn.ftz.f32 	%f51, %f43, %f8, %f50;
	ld.const.f32 	%f10, [k601YCbCr_To_RGB32f+20];
	fma.rn.ftz.f32 	%f11, %f47, %f10, %f51;
	ld.const.f32 	%f12, [k601YCbCr_To_RGB32f+24];
	ld.const.f32 	%f13, [k601YCbCr_To_RGB32f+28];
	mul.ftz.f32 	%f52, %f45, %f13;
	fma.rn.ftz.f32 	%f53, %f43, %f12, %f52;
	ld.const.f32 	%f14, [k601YCbCr_To_RGB32f+32];
	fma.rn.ftz.f32 	%f15, %f47, %f14, %f53;
	setp.eq.s32	%p4, %r12, 0;
	@%p4 bra 	BB1_3;

	cvta.to.global.u64 	%rd4, %rd2;
	mad.lo.s32 	%r35, %r2, %r11, %r21;
	add.s32 	%r36, %r35, 1;
	mul.wide.s32 	%rd5, %r36, 16;
	add.s64 	%rd6, %rd4, %rd5;
	mov.f32 	%f54, 0f3F800000;
	st.global.v4.f32 	[%rd6], {%f15, %f11, %f7, %f54};
	bra.uni 	BB1_4;

BB1_3:
	cvta.to.global.u64 	%rd7, %rd2;
	mad.lo.s32 	%r46, %r2, %r11, %r21;
	add.s32 	%r47, %r46, 1;
	mul.wide.s32 	%rd8, %r47, 8;
	add.s64 	%rd9, %rd7, %rd8;
	mov.f32 	%f55, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f55;
	mov.b16 	%rs1, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f7;
	mov.b16 	%rs2, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f11;
	mov.b16 	%rs3, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f15;
	mov.b16 	%rs4, %temp;
}
	st.global.v4.u16 	[%rd9], {%rs4, %rs3, %rs2, %rs1};

BB1_4:
	mov.b32 	 %f56, %r3;
	mov.b32 	 %f57, %r4;
	mov.b32 	 %f58, %r7;
	mul.ftz.f32 	%f59, %f58, 0f437F0000;
	sub.ftz.f32 	%f60, %f59, %f1;
	mul.ftz.f32 	%f61, %f56, 0f437F0000;
	sub.ftz.f32 	%f62, %f61, %f2;
	mul.ftz.f32 	%f63, %f57, 0f437F0000;
	sub.ftz.f32 	%f64, %f63, %f3;
	mul.ftz.f32 	%f65, %f62, %f5;
	fma.rn.ftz.f32 	%f66, %f60, %f4, %f65;
	fma.rn.ftz.f32 	%f16, %f64, %f6, %f66;
	mul.ftz.f32 	%f67, %f62, %f9;
	fma.rn.ftz.f32 	%f68, %f60, %f8, %f67;
	fma.rn.ftz.f32 	%f17, %f64, %f10, %f68;
	mul.ftz.f32 	%f69, %f62, %f13;
	fma.rn.ftz.f32 	%f70, %f60, %f12, %f69;
	fma.rn.ftz.f32 	%f18, %f64, %f14, %f70;
	mad.lo.s32 	%r57, %r2, %r11, %r21;
	cvt.s64.s32	%rd1, %r57;
	@%p4 bra 	BB1_6;

	cvta.to.global.u64 	%rd10, %rd2;
	shl.b64 	%rd11, %rd1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	mov.f32 	%f71, 0f3F800000;
	st.global.v4.f32 	[%rd12], {%f18, %f17, %f16, %f71};
	bra.uni 	BB1_7;

BB1_6:
	cvta.to.global.u64 	%rd13, %rd2;
	shl.b64 	%rd14, %rd1, 3;
	add.s64 	%rd15, %rd13, %rd14;
	mov.f32 	%f72, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f72;
	mov.b16 	%rs5, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f16;
	mov.b16 	%rs6, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f17;
	mov.b16 	%rs7, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f18;
	mov.b16 	%rs8, %temp;
}
	st.global.v4.u16 	[%rd15], {%rs8, %rs7, %rs6, %rs5};

BB1_7:
	ret;
}

	// .globl	PixelFormatConvert_NV12_FIELD_709_Kernel
.visible .entry PixelFormatConvert_NV12_FIELD_709_Kernel(
	.param .u64 PixelFormatConvert_NV12_FIELD_709_Kernel_param_0,
	.param .u64 PixelFormatConvert_NV12_FIELD_709_Kernel_param_1,
	.param .u32 PixelFormatConvert_NV12_FIELD_709_Kernel_param_2,
	.param .u32 PixelFormatConvert_NV12_FIELD_709_Kernel_param_3,
	.param .u32 PixelFormatConvert_NV12_FIELD_709_Kernel_param_4,
	.param .u32 PixelFormatConvert_NV12_FIELD_709_Kernel_param_5
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<17>;
	.reg .f32 	%f<126>;
	.reg .b32 	%r<95>;
	.reg .b64 	%rd<30>;


	ld.param.u64 	%rd5, [PixelFormatConvert_NV12_FIELD_709_Kernel_param_1];
	ld.param.u32 	%r20, [PixelFormatConvert_NV12_FIELD_709_Kernel_param_2];
	ld.param.u32 	%r21, [PixelFormatConvert_NV12_FIELD_709_Kernel_param_3];
	ld.param.u32 	%r22, [PixelFormatConvert_NV12_FIELD_709_Kernel_param_4];
	ld.param.u32 	%r23, [PixelFormatConvert_NV12_FIELD_709_Kernel_param_5];
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %ntid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r1, %r25, %r24, %r26;
	mov.u32 	%r27, %ntid.y;
	mov.u32 	%r28, %ctaid.y;
	mov.u32 	%r29, %tid.y;
	mad.lo.s32 	%r2, %r27, %r28, %r29;
	shl.b32 	%r30, %r1, 1;
	setp.ge.u32	%p1, %r30, %r22;
	setp.ge.u32	%p2, %r2, %r23;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB2_13;

	cvt.rn.f32.u32	%f1, %r2;
	add.ftz.f32 	%f30, %f1, 0fBE000000;
	mov.f32 	%f31, 0f00000000;
	max.ftz.f32 	%f32, %f30, %f31;
	cvt.rn.f32.u32	%f2, %r23;
	min.ftz.f32 	%f33, %f32, %f2;
	fma.rn.ftz.f32 	%f34, %f33, 0f3F000000, %f2;
	add.ftz.f32 	%f35, %f34, 0f3F000000;
	cvt.rn.f32.u32	%f3, %r1;
	add.ftz.f32 	%f36, %f3, 0f3F000000;
	cvt.rn.f32.u32	%f4, %r22;
	min.ftz.f32 	%f5, %f36, %f4;
	add.ftz.f32 	%f37, %f5, 0f00000000;
	add.ftz.f32 	%f38, %f37, 0f3F000000;
	tex.2d.v4.u32.f32	{%r31, %r32, %r33, %r34}, [inTexture, {%f38, %f35}];
	mov.b32 	 %f39, %r31;
	mov.b32 	 %f40, %r32;
	tex.2d.v4.u32.f32	{%r3, %r4, %r5, %r6}, [inTexture, {%f36, %f35}];
	add.ftz.f32 	%f41, %f1, 0f3F000000;
	tex.2d.v4.u32.f32	{%r7, %r8, %r9, %r10}, [inTexture, {%f36, %f41}];
	mov.b32 	 %f42, %r8;
	mul.ftz.f32 	%f43, %f40, 0f437F0000;
	mul.ftz.f32 	%f44, %f39, 0f437F0000;
	mul.ftz.f32 	%f45, %f42, 0f437F0000;
	ld.const.f32 	%f46, [kYCbCrOffset];
	mov.f32 	%f47, 0f437F0000;
	div.approx.ftz.f32 	%f48, %f47, %f47;
	mul.ftz.f32 	%f6, %f48, %f46;
	sub.ftz.f32 	%f49, %f45, %f6;
	ld.const.f32 	%f50, [kYCbCrOffset+4];
	mul.ftz.f32 	%f7, %f48, %f50;
	sub.ftz.f32 	%f51, %f44, %f7;
	ld.const.f32 	%f52, [kYCbCrOffset+8];
	mul.ftz.f32 	%f8, %f48, %f52;
	sub.ftz.f32 	%f53, %f43, %f8;
	ld.const.f32 	%f9, [k709YCbCr_To_RGB32f];
	ld.const.f32 	%f10, [k709YCbCr_To_RGB32f+4];
	mul.ftz.f32 	%f54, %f51, %f10;
	fma.rn.ftz.f32 	%f55, %f49, %f9, %f54;
	ld.const.f32 	%f11, [k709YCbCr_To_RGB32f+8];
	fma.rn.ftz.f32 	%f12, %f53, %f11, %f55;
	ld.const.f32 	%f13, [k709YCbCr_To_RGB32f+12];
	ld.const.f32 	%f14, [k709YCbCr_To_RGB32f+16];
	mul.ftz.f32 	%f56, %f51, %f14;
	fma.rn.ftz.f32 	%f57, %f49, %f13, %f56;
	ld.const.f32 	%f15, [k709YCbCr_To_RGB32f+20];
	fma.rn.ftz.f32 	%f16, %f53, %f15, %f57;
	ld.const.f32 	%f17, [k709YCbCr_To_RGB32f+24];
	ld.const.f32 	%f18, [k709YCbCr_To_RGB32f+28];
	mul.ftz.f32 	%f58, %f51, %f18;
	fma.rn.ftz.f32 	%f59, %f49, %f17, %f58;
	ld.const.f32 	%f19, [k709YCbCr_To_RGB32f+32];
	fma.rn.ftz.f32 	%f20, %f53, %f19, %f59;
	setp.eq.s32	%p4, %r21, 0;
	@%p4 bra 	BB2_3;

	cvta.to.global.u64 	%rd6, %rd5;
	mul.lo.s32 	%r39, %r2, %r20;
	shl.b32 	%r40, %r39, 1;
	add.s32 	%r46, %r30, %r40;
	add.s32 	%r47, %r46, 1;
	mul.wide.s32 	%rd7, %r47, 16;
	add.s64 	%rd8, %rd6, %rd7;
	mov.f32 	%f60, 0f3F800000;
	st.global.v4.f32 	[%rd8], {%f20, %f16, %f12, %f60};
	bra.uni 	BB2_4;

BB2_3:
	cvta.to.global.u64 	%rd9, %rd5;
	mul.lo.s32 	%r52, %r2, %r20;
	shl.b32 	%r53, %r52, 1;
	add.s32 	%r59, %r30, %r53;
	add.s32 	%r60, %r59, 1;
	mul.wide.s32 	%rd10, %r60, 8;
	add.s64 	%rd11, %rd9, %rd10;
	mov.f32 	%f61, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f61;
	mov.b16 	%rs1, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f12;
	mov.b16 	%rs2, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f16;
	mov.b16 	%rs3, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f20;
	mov.b16 	%rs4, %temp;
}
	st.global.v4.u16 	[%rd11], {%rs4, %rs3, %rs2, %rs1};

BB2_4:
	mov.b32 	 %f62, %r3;
	mov.b32 	 %f63, %r4;
	mov.b32 	 %f64, %r7;
	mul.ftz.f32 	%f65, %f64, 0f437F0000;
	sub.ftz.f32 	%f66, %f65, %f6;
	mul.ftz.f32 	%f67, %f62, 0f437F0000;
	sub.ftz.f32 	%f68, %f67, %f7;
	mul.ftz.f32 	%f69, %f63, 0f437F0000;
	sub.ftz.f32 	%f70, %f69, %f8;
	mul.ftz.f32 	%f71, %f68, %f10;
	fma.rn.ftz.f32 	%f72, %f66, %f9, %f71;
	fma.rn.ftz.f32 	%f21, %f70, %f11, %f72;
	mul.ftz.f32 	%f73, %f68, %f14;
	fma.rn.ftz.f32 	%f74, %f66, %f13, %f73;
	fma.rn.ftz.f32 	%f22, %f70, %f15, %f74;
	mul.ftz.f32 	%f75, %f68, %f18;
	fma.rn.ftz.f32 	%f76, %f66, %f17, %f75;
	fma.rn.ftz.f32 	%f23, %f70, %f19, %f76;
	shl.b32 	%r70, %r2, 1;
	mad.lo.s32 	%r71, %r70, %r20, %r30;
	cvt.s64.s32	%rd2, %r71;
	@%p4 bra 	BB2_6;

	cvta.to.global.u64 	%rd12, %rd5;
	shl.b64 	%rd13, %rd2, 4;
	add.s64 	%rd14, %rd12, %rd13;
	mov.f32 	%f77, 0f3F800000;
	st.global.v4.f32 	[%rd14], {%f23, %f22, %f21, %f77};
	bra.uni 	BB2_7;

BB2_6:
	cvta.to.global.u64 	%rd15, %rd5;
	shl.b64 	%rd16, %rd2, 3;
	add.s64 	%rd17, %rd15, %rd16;
	mov.f32 	%f78, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f78;
	mov.b16 	%rs5, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f21;
	mov.b16 	%rs6, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f22;
	mov.b16 	%rs7, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f23;
	mov.b16 	%rs8, %temp;
}
	st.global.v4.u16 	[%rd17], {%rs8, %rs7, %rs6, %rs5};

BB2_7:
	add.ftz.f32 	%f79, %f1, 0f3E000000;
	max.ftz.f32 	%f81, %f79, %f31;
	min.ftz.f32 	%f82, %f81, %f2;
	fma.rn.ftz.f32 	%f83, %f82, 0f3F000000, %f2;
	add.ftz.f32 	%f84, %f83, 0f3F000000;
	mov.f32 	%f85, 0f40000000;
	div.approx.ftz.f32 	%f86, %f4, %f85;
	add.ftz.f32 	%f87, %f5, %f86;
	add.ftz.f32 	%f88, %f87, 0f3F000000;
	tex.2d.v4.u32.f32	{%r72, %r73, %r74, %r75}, [inTexture, {%f88, %f84}];
	mov.b32 	 %f89, %r72;
	mov.b32 	 %f90, %r73;
	add.ftz.f32 	%f91, %f3, %f86;
	add.ftz.f32 	%f92, %f91, 0f3F000000;
	tex.2d.v4.u32.f32	{%r11, %r12, %r13, %r14}, [inTexture, {%f92, %f84}];
	tex.2d.v4.u32.f32	{%r15, %r16, %r17, %r18}, [inTexture, {%f92, %f41}];
	add.s32 	%r81, %r70, 1;
	mov.b32 	 %f94, %r16;
	mul.ftz.f32 	%f95, %f90, 0f437F0000;
	mul.ftz.f32 	%f96, %f89, 0f437F0000;
	mul.ftz.f32 	%f97, %f94, 0f437F0000;
	sub.ftz.f32 	%f98, %f97, %f6;
	sub.ftz.f32 	%f99, %f96, %f7;
	sub.ftz.f32 	%f100, %f95, %f8;
	mul.ftz.f32 	%f101, %f99, %f10;
	fma.rn.ftz.f32 	%f102, %f98, %f9, %f101;
	fma.rn.ftz.f32 	%f24, %f100, %f11, %f102;
	mul.ftz.f32 	%f103, %f99, %f14;
	fma.rn.ftz.f32 	%f104, %f98, %f13, %f103;
	fma.rn.ftz.f32 	%f25, %f100, %f15, %f104;
	mul.ftz.f32 	%f105, %f99, %f18;
	fma.rn.ftz.f32 	%f106, %f98, %f17, %f105;
	fma.rn.ftz.f32 	%f26, %f100, %f19, %f106;
	mul.lo.s32 	%r19, %r81, %r20;
	add.s32 	%r87, %r30, %r19;
	add.s32 	%r88, %r87, 1;
	cvt.s64.s32	%rd3, %r88;
	@%p4 bra 	BB2_9;

	cvta.to.global.u64 	%rd18, %rd5;
	shl.b64 	%rd19, %rd3, 4;
	add.s64 	%rd20, %rd18, %rd19;
	mov.f32 	%f107, 0f3F800000;
	st.global.v4.f32 	[%rd20], {%f26, %f25, %f24, %f107};
	bra.uni 	BB2_10;

BB2_9:
	cvta.to.global.u64 	%rd21, %rd5;
	shl.b64 	%rd22, %rd3, 3;
	add.s64 	%rd23, %rd21, %rd22;
	mov.f32 	%f108, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f108;
	mov.b16 	%rs9, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f24;
	mov.b16 	%rs10, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f25;
	mov.b16 	%rs11, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f26;
	mov.b16 	%rs12, %temp;
}
	st.global.v4.u16 	[%rd23], {%rs12, %rs11, %rs10, %rs9};

BB2_10:
	mov.b32 	 %f109, %r11;
	mov.b32 	 %f110, %r12;
	mov.b32 	 %f111, %r15;
	mul.ftz.f32 	%f112, %f111, 0f437F0000;
	sub.ftz.f32 	%f113, %f112, %f6;
	mul.ftz.f32 	%f114, %f109, 0f437F0000;
	sub.ftz.f32 	%f115, %f114, %f7;
	mul.ftz.f32 	%f116, %f110, 0f437F0000;
	sub.ftz.f32 	%f117, %f116, %f8;
	mul.ftz.f32 	%f118, %f115, %f10;
	fma.rn.ftz.f32 	%f119, %f113, %f9, %f118;
	fma.rn.ftz.f32 	%f27, %f117, %f11, %f119;
	mul.ftz.f32 	%f120, %f115, %f14;
	fma.rn.ftz.f32 	%f121, %f113, %f13, %f120;
	fma.rn.ftz.f32 	%f28, %f117, %f15, %f121;
	mul.ftz.f32 	%f122, %f115, %f18;
	fma.rn.ftz.f32 	%f123, %f113, %f17, %f122;
	fma.rn.ftz.f32 	%f29, %f117, %f19, %f123;
	add.s32 	%r94, %r19, %r30;
	cvt.s64.s32	%rd4, %r94;
	@%p4 bra 	BB2_12;

	cvta.to.global.u64 	%rd24, %rd5;
	shl.b64 	%rd25, %rd4, 4;
	add.s64 	%rd26, %rd24, %rd25;
	mov.f32 	%f124, 0f3F800000;
	st.global.v4.f32 	[%rd26], {%f29, %f28, %f27, %f124};
	bra.uni 	BB2_13;

BB2_12:
	cvta.to.global.u64 	%rd27, %rd5;
	shl.b64 	%rd28, %rd4, 3;
	add.s64 	%rd29, %rd27, %rd28;
	mov.f32 	%f125, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f125;
	mov.b16 	%rs13, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f27;
	mov.b16 	%rs14, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f28;
	mov.b16 	%rs15, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f29;
	mov.b16 	%rs16, %temp;
}
	st.global.v4.u16 	[%rd29], {%rs16, %rs15, %rs14, %rs13};

BB2_13:
	ret;
}

	// .globl	PixelFormatConvert_NV12_FRAME_709_Kernel
.visible .entry PixelFormatConvert_NV12_FRAME_709_Kernel(
	.param .u64 PixelFormatConvert_NV12_FRAME_709_Kernel_param_0,
	.param .u64 PixelFormatConvert_NV12_FRAME_709_Kernel_param_1,
	.param .u32 PixelFormatConvert_NV12_FRAME_709_Kernel_param_2,
	.param .u32 PixelFormatConvert_NV12_FRAME_709_Kernel_param_3,
	.param .u32 PixelFormatConvert_NV12_FRAME_709_Kernel_param_4,
	.param .u32 PixelFormatConvert_NV12_FRAME_709_Kernel_param_5
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<73>;
	.reg .b32 	%r<58>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [PixelFormatConvert_NV12_FRAME_709_Kernel_param_1];
	ld.param.u32 	%r11, [PixelFormatConvert_NV12_FRAME_709_Kernel_param_2];
	ld.param.u32 	%r12, [PixelFormatConvert_NV12_FRAME_709_Kernel_param_3];
	ld.param.u32 	%r13, [PixelFormatConvert_NV12_FRAME_709_Kernel_param_4];
	ld.param.u32 	%r14, [PixelFormatConvert_NV12_FRAME_709_Kernel_param_5];
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r1, %r16, %r15, %r17;
	mov.u32 	%r18, %ntid.y;
	mov.u32 	%r19, %ctaid.y;
	mov.u32 	%r20, %tid.y;
	mad.lo.s32 	%r2, %r18, %r19, %r20;
	shl.b32 	%r21, %r1, 1;
	setp.ge.u32	%p1, %r21, %r13;
	setp.ge.u32	%p2, %r2, %r14;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB3_7;

	cvt.rn.f32.u32	%f19, %r2;
	add.ftz.f32 	%f20, %f19, 0fBE800000;
	mov.f32 	%f21, 0f00000000;
	max.ftz.f32 	%f22, %f20, %f21;
	cvt.rn.f32.u32	%f23, %r14;
	min.ftz.f32 	%f24, %f22, %f23;
	fma.rn.ftz.f32 	%f25, %f24, 0f3F000000, %f23;
	add.ftz.f32 	%f26, %f25, 0f3F000000;
	cvt.rn.f32.u32	%f27, %r1;
	add.ftz.f32 	%f28, %f27, 0f3F000000;
	cvt.rn.f32.u32	%f29, %r13;
	min.ftz.f32 	%f30, %f28, %f29;
	add.ftz.f32 	%f31, %f30, 0f00000000;
	add.ftz.f32 	%f32, %f31, 0f3F000000;
	tex.2d.v4.u32.f32	{%r22, %r23, %r24, %r25}, [inTexture, {%f32, %f26}];
	mov.b32 	 %f33, %r22;
	mov.b32 	 %f34, %r23;
	tex.2d.v4.u32.f32	{%r3, %r4, %r5, %r6}, [inTexture, {%f28, %f26}];
	add.ftz.f32 	%f35, %f19, 0f3F000000;
	tex.2d.v4.u32.f32	{%r7, %r8, %r9, %r10}, [inTexture, {%f28, %f35}];
	mov.b32 	 %f36, %r8;
	mul.ftz.f32 	%f37, %f34, 0f437F0000;
	mul.ftz.f32 	%f38, %f33, 0f437F0000;
	mul.ftz.f32 	%f39, %f36, 0f437F0000;
	ld.const.f32 	%f40, [kYCbCrOffset];
	mov.f32 	%f41, 0f437F0000;
	div.approx.ftz.f32 	%f42, %f41, %f41;
	mul.ftz.f32 	%f1, %f42, %f40;
	sub.ftz.f32 	%f43, %f39, %f1;
	ld.const.f32 	%f44, [kYCbCrOffset+4];
	mul.ftz.f32 	%f2, %f42, %f44;
	sub.ftz.f32 	%f45, %f38, %f2;
	ld.const.f32 	%f46, [kYCbCrOffset+8];
	mul.ftz.f32 	%f3, %f42, %f46;
	sub.ftz.f32 	%f47, %f37, %f3;
	ld.const.f32 	%f4, [k709YCbCr_To_RGB32f];
	ld.const.f32 	%f5, [k709YCbCr_To_RGB32f+4];
	mul.ftz.f32 	%f48, %f45, %f5;
	fma.rn.ftz.f32 	%f49, %f43, %f4, %f48;
	ld.const.f32 	%f6, [k709YCbCr_To_RGB32f+8];
	fma.rn.ftz.f32 	%f7, %f47, %f6, %f49;
	ld.const.f32 	%f8, [k709YCbCr_To_RGB32f+12];
	ld.const.f32 	%f9, [k709YCbCr_To_RGB32f+16];
	mul.ftz.f32 	%f50, %f45, %f9;
	fma.rn.ftz.f32 	%f51, %f43, %f8, %f50;
	ld.const.f32 	%f10, [k709YCbCr_To_RGB32f+20];
	fma.rn.ftz.f32 	%f11, %f47, %f10, %f51;
	ld.const.f32 	%f12, [k709YCbCr_To_RGB32f+24];
	ld.const.f32 	%f13, [k709YCbCr_To_RGB32f+28];
	mul.ftz.f32 	%f52, %f45, %f13;
	fma.rn.ftz.f32 	%f53, %f43, %f12, %f52;
	ld.const.f32 	%f14, [k709YCbCr_To_RGB32f+32];
	fma.rn.ftz.f32 	%f15, %f47, %f14, %f53;
	setp.eq.s32	%p4, %r12, 0;
	@%p4 bra 	BB3_3;

	cvta.to.global.u64 	%rd4, %rd2;
	mad.lo.s32 	%r35, %r2, %r11, %r21;
	add.s32 	%r36, %r35, 1;
	mul.wide.s32 	%rd5, %r36, 16;
	add.s64 	%rd6, %rd4, %rd5;
	mov.f32 	%f54, 0f3F800000;
	st.global.v4.f32 	[%rd6], {%f15, %f11, %f7, %f54};
	bra.uni 	BB3_4;

BB3_3:
	cvta.to.global.u64 	%rd7, %rd2;
	mad.lo.s32 	%r46, %r2, %r11, %r21;
	add.s32 	%r47, %r46, 1;
	mul.wide.s32 	%rd8, %r47, 8;
	add.s64 	%rd9, %rd7, %rd8;
	mov.f32 	%f55, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f55;
	mov.b16 	%rs1, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f7;
	mov.b16 	%rs2, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f11;
	mov.b16 	%rs3, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f15;
	mov.b16 	%rs4, %temp;
}
	st.global.v4.u16 	[%rd9], {%rs4, %rs3, %rs2, %rs1};

BB3_4:
	mov.b32 	 %f56, %r3;
	mov.b32 	 %f57, %r4;
	mov.b32 	 %f58, %r7;
	mul.ftz.f32 	%f59, %f58, 0f437F0000;
	sub.ftz.f32 	%f60, %f59, %f1;
	mul.ftz.f32 	%f61, %f56, 0f437F0000;
	sub.ftz.f32 	%f62, %f61, %f2;
	mul.ftz.f32 	%f63, %f57, 0f437F0000;
	sub.ftz.f32 	%f64, %f63, %f3;
	mul.ftz.f32 	%f65, %f62, %f5;
	fma.rn.ftz.f32 	%f66, %f60, %f4, %f65;
	fma.rn.ftz.f32 	%f16, %f64, %f6, %f66;
	mul.ftz.f32 	%f67, %f62, %f9;
	fma.rn.ftz.f32 	%f68, %f60, %f8, %f67;
	fma.rn.ftz.f32 	%f17, %f64, %f10, %f68;
	mul.ftz.f32 	%f69, %f62, %f13;
	fma.rn.ftz.f32 	%f70, %f60, %f12, %f69;
	fma.rn.ftz.f32 	%f18, %f64, %f14, %f70;
	mad.lo.s32 	%r57, %r2, %r11, %r21;
	cvt.s64.s32	%rd1, %r57;
	@%p4 bra 	BB3_6;

	cvta.to.global.u64 	%rd10, %rd2;
	shl.b64 	%rd11, %rd1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	mov.f32 	%f71, 0f3F800000;
	st.global.v4.f32 	[%rd12], {%f18, %f17, %f16, %f71};
	bra.uni 	BB3_7;

BB3_6:
	cvta.to.global.u64 	%rd13, %rd2;
	shl.b64 	%rd14, %rd1, 3;
	add.s64 	%rd15, %rd13, %rd14;
	mov.f32 	%f72, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f72;
	mov.b16 	%rs5, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f16;
	mov.b16 	%rs6, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f17;
	mov.b16 	%rs7, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f18;
	mov.b16 	%rs8, %temp;
}
	st.global.v4.u16 	[%rd15], {%rs8, %rs7, %rs6, %rs5};

BB3_7:
	ret;
}

	// .globl	PixelFormatConvert_NV12_FIELD_601_FullRange_Kernel
.visible .entry PixelFormatConvert_NV12_FIELD_601_FullRange_Kernel(
	.param .u64 PixelFormatConvert_NV12_FIELD_601_FullRange_Kernel_param_0,
	.param .u64 PixelFormatConvert_NV12_FIELD_601_FullRange_Kernel_param_1,
	.param .u32 PixelFormatConvert_NV12_FIELD_601_FullRange_Kernel_param_2,
	.param .u32 PixelFormatConvert_NV12_FIELD_601_FullRange_Kernel_param_3,
	.param .u32 PixelFormatConvert_NV12_FIELD_601_FullRange_Kernel_param_4,
	.param .u32 PixelFormatConvert_NV12_FIELD_601_FullRange_Kernel_param_5
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<17>;
	.reg .f32 	%f<126>;
	.reg .b32 	%r<95>;
	.reg .b64 	%rd<30>;


	ld.param.u64 	%rd5, [PixelFormatConvert_NV12_FIELD_601_FullRange_Kernel_param_1];
	ld.param.u32 	%r20, [PixelFormatConvert_NV12_FIELD_601_FullRange_Kernel_param_2];
	ld.param.u32 	%r21, [PixelFormatConvert_NV12_FIELD_601_FullRange_Kernel_param_3];
	ld.param.u32 	%r22, [PixelFormatConvert_NV12_FIELD_601_FullRange_Kernel_param_4];
	ld.param.u32 	%r23, [PixelFormatConvert_NV12_FIELD_601_FullRange_Kernel_param_5];
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %ntid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r1, %r25, %r24, %r26;
	mov.u32 	%r27, %ntid.y;
	mov.u32 	%r28, %ctaid.y;
	mov.u32 	%r29, %tid.y;
	mad.lo.s32 	%r2, %r27, %r28, %r29;
	shl.b32 	%r30, %r1, 1;
	setp.ge.u32	%p1, %r30, %r22;
	setp.ge.u32	%p2, %r2, %r23;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB4_13;

	cvt.rn.f32.u32	%f1, %r2;
	add.ftz.f32 	%f30, %f1, 0fBE000000;
	mov.f32 	%f31, 0f00000000;
	max.ftz.f32 	%f32, %f30, %f31;
	cvt.rn.f32.u32	%f2, %r23;
	min.ftz.f32 	%f33, %f32, %f2;
	fma.rn.ftz.f32 	%f34, %f33, 0f3F000000, %f2;
	add.ftz.f32 	%f35, %f34, 0f3F000000;
	cvt.rn.f32.u32	%f3, %r1;
	add.ftz.f32 	%f36, %f3, 0f3F000000;
	cvt.rn.f32.u32	%f4, %r22;
	min.ftz.f32 	%f5, %f36, %f4;
	add.ftz.f32 	%f37, %f5, 0f00000000;
	add.ftz.f32 	%f38, %f37, 0f3F000000;
	tex.2d.v4.u32.f32	{%r31, %r32, %r33, %r34}, [inTexture, {%f38, %f35}];
	mov.b32 	 %f39, %r31;
	mov.b32 	 %f40, %r32;
	tex.2d.v4.u32.f32	{%r3, %r4, %r5, %r6}, [inTexture, {%f36, %f35}];
	add.ftz.f32 	%f41, %f1, 0f3F000000;
	tex.2d.v4.u32.f32	{%r7, %r8, %r9, %r10}, [inTexture, {%f36, %f41}];
	mov.b32 	 %f42, %r8;
	mul.ftz.f32 	%f43, %f40, 0f437F0000;
	mul.ftz.f32 	%f44, %f39, 0f437F0000;
	mul.ftz.f32 	%f45, %f42, 0f437F0000;
	ld.const.f32 	%f46, [kYCbCrFullRangeOffset];
	mov.f32 	%f47, 0f437F0000;
	div.approx.ftz.f32 	%f48, %f47, %f47;
	mul.ftz.f32 	%f6, %f48, %f46;
	sub.ftz.f32 	%f49, %f45, %f6;
	ld.const.f32 	%f50, [kYCbCrFullRangeOffset+4];
	mul.ftz.f32 	%f7, %f48, %f50;
	sub.ftz.f32 	%f51, %f44, %f7;
	ld.const.f32 	%f52, [kYCbCrFullRangeOffset+8];
	mul.ftz.f32 	%f8, %f48, %f52;
	sub.ftz.f32 	%f53, %f43, %f8;
	ld.const.f32 	%f9, [k601YCbCrFullRange_To_RGB32f];
	ld.const.f32 	%f10, [k601YCbCrFullRange_To_RGB32f+4];
	mul.ftz.f32 	%f54, %f51, %f10;
	fma.rn.ftz.f32 	%f55, %f49, %f9, %f54;
	ld.const.f32 	%f11, [k601YCbCrFullRange_To_RGB32f+8];
	fma.rn.ftz.f32 	%f12, %f53, %f11, %f55;
	ld.const.f32 	%f13, [k601YCbCrFullRange_To_RGB32f+12];
	ld.const.f32 	%f14, [k601YCbCrFullRange_To_RGB32f+16];
	mul.ftz.f32 	%f56, %f51, %f14;
	fma.rn.ftz.f32 	%f57, %f49, %f13, %f56;
	ld.const.f32 	%f15, [k601YCbCrFullRange_To_RGB32f+20];
	fma.rn.ftz.f32 	%f16, %f53, %f15, %f57;
	ld.const.f32 	%f17, [k601YCbCrFullRange_To_RGB32f+24];
	ld.const.f32 	%f18, [k601YCbCrFullRange_To_RGB32f+28];
	mul.ftz.f32 	%f58, %f51, %f18;
	fma.rn.ftz.f32 	%f59, %f49, %f17, %f58;
	ld.const.f32 	%f19, [k601YCbCrFullRange_To_RGB32f+32];
	fma.rn.ftz.f32 	%f20, %f53, %f19, %f59;
	setp.eq.s32	%p4, %r21, 0;
	@%p4 bra 	BB4_3;

	cvta.to.global.u64 	%rd6, %rd5;
	mul.lo.s32 	%r39, %r2, %r20;
	shl.b32 	%r40, %r39, 1;
	add.s32 	%r46, %r30, %r40;
	add.s32 	%r47, %r46, 1;
	mul.wide.s32 	%rd7, %r47, 16;
	add.s64 	%rd8, %rd6, %rd7;
	mov.f32 	%f60, 0f3F800000;
	st.global.v4.f32 	[%rd8], {%f20, %f16, %f12, %f60};
	bra.uni 	BB4_4;

BB4_3:
	cvta.to.global.u64 	%rd9, %rd5;
	mul.lo.s32 	%r52, %r2, %r20;
	shl.b32 	%r53, %r52, 1;
	add.s32 	%r59, %r30, %r53;
	add.s32 	%r60, %r59, 1;
	mul.wide.s32 	%rd10, %r60, 8;
	add.s64 	%rd11, %rd9, %rd10;
	mov.f32 	%f61, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f61;
	mov.b16 	%rs1, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f12;
	mov.b16 	%rs2, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f16;
	mov.b16 	%rs3, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f20;
	mov.b16 	%rs4, %temp;
}
	st.global.v4.u16 	[%rd11], {%rs4, %rs3, %rs2, %rs1};

BB4_4:
	mov.b32 	 %f62, %r3;
	mov.b32 	 %f63, %r4;
	mov.b32 	 %f64, %r7;
	mul.ftz.f32 	%f65, %f64, 0f437F0000;
	sub.ftz.f32 	%f66, %f65, %f6;
	mul.ftz.f32 	%f67, %f62, 0f437F0000;
	sub.ftz.f32 	%f68, %f67, %f7;
	mul.ftz.f32 	%f69, %f63, 0f437F0000;
	sub.ftz.f32 	%f70, %f69, %f8;
	mul.ftz.f32 	%f71, %f68, %f10;
	fma.rn.ftz.f32 	%f72, %f66, %f9, %f71;
	fma.rn.ftz.f32 	%f21, %f70, %f11, %f72;
	mul.ftz.f32 	%f73, %f68, %f14;
	fma.rn.ftz.f32 	%f74, %f66, %f13, %f73;
	fma.rn.ftz.f32 	%f22, %f70, %f15, %f74;
	mul.ftz.f32 	%f75, %f68, %f18;
	fma.rn.ftz.f32 	%f76, %f66, %f17, %f75;
	fma.rn.ftz.f32 	%f23, %f70, %f19, %f76;
	shl.b32 	%r70, %r2, 1;
	mad.lo.s32 	%r71, %r70, %r20, %r30;
	cvt.s64.s32	%rd2, %r71;
	@%p4 bra 	BB4_6;

	cvta.to.global.u64 	%rd12, %rd5;
	shl.b64 	%rd13, %rd2, 4;
	add.s64 	%rd14, %rd12, %rd13;
	mov.f32 	%f77, 0f3F800000;
	st.global.v4.f32 	[%rd14], {%f23, %f22, %f21, %f77};
	bra.uni 	BB4_7;

BB4_6:
	cvta.to.global.u64 	%rd15, %rd5;
	shl.b64 	%rd16, %rd2, 3;
	add.s64 	%rd17, %rd15, %rd16;
	mov.f32 	%f78, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f78;
	mov.b16 	%rs5, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f21;
	mov.b16 	%rs6, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f22;
	mov.b16 	%rs7, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f23;
	mov.b16 	%rs8, %temp;
}
	st.global.v4.u16 	[%rd17], {%rs8, %rs7, %rs6, %rs5};

BB4_7:
	add.ftz.f32 	%f79, %f1, 0f3E000000;
	max.ftz.f32 	%f81, %f79, %f31;
	min.ftz.f32 	%f82, %f81, %f2;
	fma.rn.ftz.f32 	%f83, %f82, 0f3F000000, %f2;
	add.ftz.f32 	%f84, %f83, 0f3F000000;
	mov.f32 	%f85, 0f40000000;
	div.approx.ftz.f32 	%f86, %f4, %f85;
	add.ftz.f32 	%f87, %f5, %f86;
	add.ftz.f32 	%f88, %f87, 0f3F000000;
	tex.2d.v4.u32.f32	{%r72, %r73, %r74, %r75}, [inTexture, {%f88, %f84}];
	mov.b32 	 %f89, %r72;
	mov.b32 	 %f90, %r73;
	add.ftz.f32 	%f91, %f3, %f86;
	add.ftz.f32 	%f92, %f91, 0f3F000000;
	tex.2d.v4.u32.f32	{%r11, %r12, %r13, %r14}, [inTexture, {%f92, %f84}];
	tex.2d.v4.u32.f32	{%r15, %r16, %r17, %r18}, [inTexture, {%f92, %f41}];
	add.s32 	%r81, %r70, 1;
	mov.b32 	 %f94, %r16;
	mul.ftz.f32 	%f95, %f90, 0f437F0000;
	mul.ftz.f32 	%f96, %f89, 0f437F0000;
	mul.ftz.f32 	%f97, %f94, 0f437F0000;
	sub.ftz.f32 	%f98, %f97, %f6;
	sub.ftz.f32 	%f99, %f96, %f7;
	sub.ftz.f32 	%f100, %f95, %f8;
	mul.ftz.f32 	%f101, %f99, %f10;
	fma.rn.ftz.f32 	%f102, %f98, %f9, %f101;
	fma.rn.ftz.f32 	%f24, %f100, %f11, %f102;
	mul.ftz.f32 	%f103, %f99, %f14;
	fma.rn.ftz.f32 	%f104, %f98, %f13, %f103;
	fma.rn.ftz.f32 	%f25, %f100, %f15, %f104;
	mul.ftz.f32 	%f105, %f99, %f18;
	fma.rn.ftz.f32 	%f106, %f98, %f17, %f105;
	fma.rn.ftz.f32 	%f26, %f100, %f19, %f106;
	mul.lo.s32 	%r19, %r81, %r20;
	add.s32 	%r87, %r30, %r19;
	add.s32 	%r88, %r87, 1;
	cvt.s64.s32	%rd3, %r88;
	@%p4 bra 	BB4_9;

	cvta.to.global.u64 	%rd18, %rd5;
	shl.b64 	%rd19, %rd3, 4;
	add.s64 	%rd20, %rd18, %rd19;
	mov.f32 	%f107, 0f3F800000;
	st.global.v4.f32 	[%rd20], {%f26, %f25, %f24, %f107};
	bra.uni 	BB4_10;

BB4_9:
	cvta.to.global.u64 	%rd21, %rd5;
	shl.b64 	%rd22, %rd3, 3;
	add.s64 	%rd23, %rd21, %rd22;
	mov.f32 	%f108, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f108;
	mov.b16 	%rs9, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f24;
	mov.b16 	%rs10, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f25;
	mov.b16 	%rs11, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f26;
	mov.b16 	%rs12, %temp;
}
	st.global.v4.u16 	[%rd23], {%rs12, %rs11, %rs10, %rs9};

BB4_10:
	mov.b32 	 %f109, %r11;
	mov.b32 	 %f110, %r12;
	mov.b32 	 %f111, %r15;
	mul.ftz.f32 	%f112, %f111, 0f437F0000;
	sub.ftz.f32 	%f113, %f112, %f6;
	mul.ftz.f32 	%f114, %f109, 0f437F0000;
	sub.ftz.f32 	%f115, %f114, %f7;
	mul.ftz.f32 	%f116, %f110, 0f437F0000;
	sub.ftz.f32 	%f117, %f116, %f8;
	mul.ftz.f32 	%f118, %f115, %f10;
	fma.rn.ftz.f32 	%f119, %f113, %f9, %f118;
	fma.rn.ftz.f32 	%f27, %f117, %f11, %f119;
	mul.ftz.f32 	%f120, %f115, %f14;
	fma.rn.ftz.f32 	%f121, %f113, %f13, %f120;
	fma.rn.ftz.f32 	%f28, %f117, %f15, %f121;
	mul.ftz.f32 	%f122, %f115, %f18;
	fma.rn.ftz.f32 	%f123, %f113, %f17, %f122;
	fma.rn.ftz.f32 	%f29, %f117, %f19, %f123;
	add.s32 	%r94, %r19, %r30;
	cvt.s64.s32	%rd4, %r94;
	@%p4 bra 	BB4_12;

	cvta.to.global.u64 	%rd24, %rd5;
	shl.b64 	%rd25, %rd4, 4;
	add.s64 	%rd26, %rd24, %rd25;
	mov.f32 	%f124, 0f3F800000;
	st.global.v4.f32 	[%rd26], {%f29, %f28, %f27, %f124};
	bra.uni 	BB4_13;

BB4_12:
	cvta.to.global.u64 	%rd27, %rd5;
	shl.b64 	%rd28, %rd4, 3;
	add.s64 	%rd29, %rd27, %rd28;
	mov.f32 	%f125, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f125;
	mov.b16 	%rs13, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f27;
	mov.b16 	%rs14, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f28;
	mov.b16 	%rs15, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f29;
	mov.b16 	%rs16, %temp;
}
	st.global.v4.u16 	[%rd29], {%rs16, %rs15, %rs14, %rs13};

BB4_13:
	ret;
}

	// .globl	PixelFormatConvert_NV12_FRAME_601_FullRange_Kernel
.visible .entry PixelFormatConvert_NV12_FRAME_601_FullRange_Kernel(
	.param .u64 PixelFormatConvert_NV12_FRAME_601_FullRange_Kernel_param_0,
	.param .u64 PixelFormatConvert_NV12_FRAME_601_FullRange_Kernel_param_1,
	.param .u32 PixelFormatConvert_NV12_FRAME_601_FullRange_Kernel_param_2,
	.param .u32 PixelFormatConvert_NV12_FRAME_601_FullRange_Kernel_param_3,
	.param .u32 PixelFormatConvert_NV12_FRAME_601_FullRange_Kernel_param_4,
	.param .u32 PixelFormatConvert_NV12_FRAME_601_FullRange_Kernel_param_5
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<73>;
	.reg .b32 	%r<58>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [PixelFormatConvert_NV12_FRAME_601_FullRange_Kernel_param_1];
	ld.param.u32 	%r11, [PixelFormatConvert_NV12_FRAME_601_FullRange_Kernel_param_2];
	ld.param.u32 	%r12, [PixelFormatConvert_NV12_FRAME_601_FullRange_Kernel_param_3];
	ld.param.u32 	%r13, [PixelFormatConvert_NV12_FRAME_601_FullRange_Kernel_param_4];
	ld.param.u32 	%r14, [PixelFormatConvert_NV12_FRAME_601_FullRange_Kernel_param_5];
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r1, %r16, %r15, %r17;
	mov.u32 	%r18, %ntid.y;
	mov.u32 	%r19, %ctaid.y;
	mov.u32 	%r20, %tid.y;
	mad.lo.s32 	%r2, %r18, %r19, %r20;
	shl.b32 	%r21, %r1, 1;
	setp.ge.u32	%p1, %r21, %r13;
	setp.ge.u32	%p2, %r2, %r14;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB5_7;

	cvt.rn.f32.u32	%f19, %r2;
	add.ftz.f32 	%f20, %f19, 0fBE800000;
	mov.f32 	%f21, 0f00000000;
	max.ftz.f32 	%f22, %f20, %f21;
	cvt.rn.f32.u32	%f23, %r14;
	min.ftz.f32 	%f24, %f22, %f23;
	fma.rn.ftz.f32 	%f25, %f24, 0f3F000000, %f23;
	add.ftz.f32 	%f26, %f25, 0f3F000000;
	cvt.rn.f32.u32	%f27, %r1;
	add.ftz.f32 	%f28, %f27, 0f3F000000;
	cvt.rn.f32.u32	%f29, %r13;
	min.ftz.f32 	%f30, %f28, %f29;
	add.ftz.f32 	%f31, %f30, 0f00000000;
	add.ftz.f32 	%f32, %f31, 0f3F000000;
	tex.2d.v4.u32.f32	{%r22, %r23, %r24, %r25}, [inTexture, {%f32, %f26}];
	mov.b32 	 %f33, %r22;
	mov.b32 	 %f34, %r23;
	tex.2d.v4.u32.f32	{%r3, %r4, %r5, %r6}, [inTexture, {%f28, %f26}];
	add.ftz.f32 	%f35, %f19, 0f3F000000;
	tex.2d.v4.u32.f32	{%r7, %r8, %r9, %r10}, [inTexture, {%f28, %f35}];
	mov.b32 	 %f36, %r8;
	mul.ftz.f32 	%f37, %f34, 0f437F0000;
	mul.ftz.f32 	%f38, %f33, 0f437F0000;
	mul.ftz.f32 	%f39, %f36, 0f437F0000;
	ld.const.f32 	%f40, [kYCbCrFullRangeOffset];
	mov.f32 	%f41, 0f437F0000;
	div.approx.ftz.f32 	%f42, %f41, %f41;
	mul.ftz.f32 	%f1, %f42, %f40;
	sub.ftz.f32 	%f43, %f39, %f1;
	ld.const.f32 	%f44, [kYCbCrFullRangeOffset+4];
	mul.ftz.f32 	%f2, %f42, %f44;
	sub.ftz.f32 	%f45, %f38, %f2;
	ld.const.f32 	%f46, [kYCbCrFullRangeOffset+8];
	mul.ftz.f32 	%f3, %f42, %f46;
	sub.ftz.f32 	%f47, %f37, %f3;
	ld.const.f32 	%f4, [k601YCbCrFullRange_To_RGB32f];
	ld.const.f32 	%f5, [k601YCbCrFullRange_To_RGB32f+4];
	mul.ftz.f32 	%f48, %f45, %f5;
	fma.rn.ftz.f32 	%f49, %f43, %f4, %f48;
	ld.const.f32 	%f6, [k601YCbCrFullRange_To_RGB32f+8];
	fma.rn.ftz.f32 	%f7, %f47, %f6, %f49;
	ld.const.f32 	%f8, [k601YCbCrFullRange_To_RGB32f+12];
	ld.const.f32 	%f9, [k601YCbCrFullRange_To_RGB32f+16];
	mul.ftz.f32 	%f50, %f45, %f9;
	fma.rn.ftz.f32 	%f51, %f43, %f8, %f50;
	ld.const.f32 	%f10, [k601YCbCrFullRange_To_RGB32f+20];
	fma.rn.ftz.f32 	%f11, %f47, %f10, %f51;
	ld.const.f32 	%f12, [k601YCbCrFullRange_To_RGB32f+24];
	ld.const.f32 	%f13, [k601YCbCrFullRange_To_RGB32f+28];
	mul.ftz.f32 	%f52, %f45, %f13;
	fma.rn.ftz.f32 	%f53, %f43, %f12, %f52;
	ld.const.f32 	%f14, [k601YCbCrFullRange_To_RGB32f+32];
	fma.rn.ftz.f32 	%f15, %f47, %f14, %f53;
	setp.eq.s32	%p4, %r12, 0;
	@%p4 bra 	BB5_3;

	cvta.to.global.u64 	%rd4, %rd2;
	mad.lo.s32 	%r35, %r2, %r11, %r21;
	add.s32 	%r36, %r35, 1;
	mul.wide.s32 	%rd5, %r36, 16;
	add.s64 	%rd6, %rd4, %rd5;
	mov.f32 	%f54, 0f3F800000;
	st.global.v4.f32 	[%rd6], {%f15, %f11, %f7, %f54};
	bra.uni 	BB5_4;

BB5_3:
	cvta.to.global.u64 	%rd7, %rd2;
	mad.lo.s32 	%r46, %r2, %r11, %r21;
	add.s32 	%r47, %r46, 1;
	mul.wide.s32 	%rd8, %r47, 8;
	add.s64 	%rd9, %rd7, %rd8;
	mov.f32 	%f55, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f55;
	mov.b16 	%rs1, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f7;
	mov.b16 	%rs2, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f11;
	mov.b16 	%rs3, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f15;
	mov.b16 	%rs4, %temp;
}
	st.global.v4.u16 	[%rd9], {%rs4, %rs3, %rs2, %rs1};

BB5_4:
	mov.b32 	 %f56, %r3;
	mov.b32 	 %f57, %r4;
	mov.b32 	 %f58, %r7;
	mul.ftz.f32 	%f59, %f58, 0f437F0000;
	sub.ftz.f32 	%f60, %f59, %f1;
	mul.ftz.f32 	%f61, %f56, 0f437F0000;
	sub.ftz.f32 	%f62, %f61, %f2;
	mul.ftz.f32 	%f63, %f57, 0f437F0000;
	sub.ftz.f32 	%f64, %f63, %f3;
	mul.ftz.f32 	%f65, %f62, %f5;
	fma.rn.ftz.f32 	%f66, %f60, %f4, %f65;
	fma.rn.ftz.f32 	%f16, %f64, %f6, %f66;
	mul.ftz.f32 	%f67, %f62, %f9;
	fma.rn.ftz.f32 	%f68, %f60, %f8, %f67;
	fma.rn.ftz.f32 	%f17, %f64, %f10, %f68;
	mul.ftz.f32 	%f69, %f62, %f13;
	fma.rn.ftz.f32 	%f70, %f60, %f12, %f69;
	fma.rn.ftz.f32 	%f18, %f64, %f14, %f70;
	mad.lo.s32 	%r57, %r2, %r11, %r21;
	cvt.s64.s32	%rd1, %r57;
	@%p4 bra 	BB5_6;

	cvta.to.global.u64 	%rd10, %rd2;
	shl.b64 	%rd11, %rd1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	mov.f32 	%f71, 0f3F800000;
	st.global.v4.f32 	[%rd12], {%f18, %f17, %f16, %f71};
	bra.uni 	BB5_7;

BB5_6:
	cvta.to.global.u64 	%rd13, %rd2;
	shl.b64 	%rd14, %rd1, 3;
	add.s64 	%rd15, %rd13, %rd14;
	mov.f32 	%f72, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f72;
	mov.b16 	%rs5, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f16;
	mov.b16 	%rs6, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f17;
	mov.b16 	%rs7, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f18;
	mov.b16 	%rs8, %temp;
}
	st.global.v4.u16 	[%rd15], {%rs8, %rs7, %rs6, %rs5};

BB5_7:
	ret;
}

	// .globl	PixelFormatConvert_NV12_FIELD_709_FullRange_Kernel
.visible .entry PixelFormatConvert_NV12_FIELD_709_FullRange_Kernel(
	.param .u64 PixelFormatConvert_NV12_FIELD_709_FullRange_Kernel_param_0,
	.param .u64 PixelFormatConvert_NV12_FIELD_709_FullRange_Kernel_param_1,
	.param .u32 PixelFormatConvert_NV12_FIELD_709_FullRange_Kernel_param_2,
	.param .u32 PixelFormatConvert_NV12_FIELD_709_FullRange_Kernel_param_3,
	.param .u32 PixelFormatConvert_NV12_FIELD_709_FullRange_Kernel_param_4,
	.param .u32 PixelFormatConvert_NV12_FIELD_709_FullRange_Kernel_param_5
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<17>;
	.reg .f32 	%f<126>;
	.reg .b32 	%r<95>;
	.reg .b64 	%rd<30>;


	ld.param.u64 	%rd5, [PixelFormatConvert_NV12_FIELD_709_FullRange_Kernel_param_1];
	ld.param.u32 	%r20, [PixelFormatConvert_NV12_FIELD_709_FullRange_Kernel_param_2];
	ld.param.u32 	%r21, [PixelFormatConvert_NV12_FIELD_709_FullRange_Kernel_param_3];
	ld.param.u32 	%r22, [PixelFormatConvert_NV12_FIELD_709_FullRange_Kernel_param_4];
	ld.param.u32 	%r23, [PixelFormatConvert_NV12_FIELD_709_FullRange_Kernel_param_5];
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %ntid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r1, %r25, %r24, %r26;
	mov.u32 	%r27, %ntid.y;
	mov.u32 	%r28, %ctaid.y;
	mov.u32 	%r29, %tid.y;
	mad.lo.s32 	%r2, %r27, %r28, %r29;
	shl.b32 	%r30, %r1, 1;
	setp.ge.u32	%p1, %r30, %r22;
	setp.ge.u32	%p2, %r2, %r23;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB6_13;

	cvt.rn.f32.u32	%f1, %r2;
	add.ftz.f32 	%f30, %f1, 0fBE000000;
	mov.f32 	%f31, 0f00000000;
	max.ftz.f32 	%f32, %f30, %f31;
	cvt.rn.f32.u32	%f2, %r23;
	min.ftz.f32 	%f33, %f32, %f2;
	fma.rn.ftz.f32 	%f34, %f33, 0f3F000000, %f2;
	add.ftz.f32 	%f35, %f34, 0f3F000000;
	cvt.rn.f32.u32	%f3, %r1;
	add.ftz.f32 	%f36, %f3, 0f3F000000;
	cvt.rn.f32.u32	%f4, %r22;
	min.ftz.f32 	%f5, %f36, %f4;
	add.ftz.f32 	%f37, %f5, 0f00000000;
	add.ftz.f32 	%f38, %f37, 0f3F000000;
	tex.2d.v4.u32.f32	{%r31, %r32, %r33, %r34}, [inTexture, {%f38, %f35}];
	mov.b32 	 %f39, %r31;
	mov.b32 	 %f40, %r32;
	tex.2d.v4.u32.f32	{%r3, %r4, %r5, %r6}, [inTexture, {%f36, %f35}];
	add.ftz.f32 	%f41, %f1, 0f3F000000;
	tex.2d.v4.u32.f32	{%r7, %r8, %r9, %r10}, [inTexture, {%f36, %f41}];
	mov.b32 	 %f42, %r8;
	mul.ftz.f32 	%f43, %f40, 0f437F0000;
	mul.ftz.f32 	%f44, %f39, 0f437F0000;
	mul.ftz.f32 	%f45, %f42, 0f437F0000;
	ld.const.f32 	%f46, [kYCbCrFullRangeOffset];
	mov.f32 	%f47, 0f437F0000;
	div.approx.ftz.f32 	%f48, %f47, %f47;
	mul.ftz.f32 	%f6, %f48, %f46;
	sub.ftz.f32 	%f49, %f45, %f6;
	ld.const.f32 	%f50, [kYCbCrFullRangeOffset+4];
	mul.ftz.f32 	%f7, %f48, %f50;
	sub.ftz.f32 	%f51, %f44, %f7;
	ld.const.f32 	%f52, [kYCbCrFullRangeOffset+8];
	mul.ftz.f32 	%f8, %f48, %f52;
	sub.ftz.f32 	%f53, %f43, %f8;
	ld.const.f32 	%f9, [k709YCbCrFullRange_To_RGB32f];
	ld.const.f32 	%f10, [k709YCbCrFullRange_To_RGB32f+4];
	mul.ftz.f32 	%f54, %f51, %f10;
	fma.rn.ftz.f32 	%f55, %f49, %f9, %f54;
	ld.const.f32 	%f11, [k709YCbCrFullRange_To_RGB32f+8];
	fma.rn.ftz.f32 	%f12, %f53, %f11, %f55;
	ld.const.f32 	%f13, [k709YCbCrFullRange_To_RGB32f+12];
	ld.const.f32 	%f14, [k709YCbCrFullRange_To_RGB32f+16];
	mul.ftz.f32 	%f56, %f51, %f14;
	fma.rn.ftz.f32 	%f57, %f49, %f13, %f56;
	ld.const.f32 	%f15, [k709YCbCrFullRange_To_RGB32f+20];
	fma.rn.ftz.f32 	%f16, %f53, %f15, %f57;
	ld.const.f32 	%f17, [k709YCbCrFullRange_To_RGB32f+24];
	ld.const.f32 	%f18, [k709YCbCrFullRange_To_RGB32f+28];
	mul.ftz.f32 	%f58, %f51, %f18;
	fma.rn.ftz.f32 	%f59, %f49, %f17, %f58;
	ld.const.f32 	%f19, [k709YCbCrFullRange_To_RGB32f+32];
	fma.rn.ftz.f32 	%f20, %f53, %f19, %f59;
	setp.eq.s32	%p4, %r21, 0;
	@%p4 bra 	BB6_3;

	cvta.to.global.u64 	%rd6, %rd5;
	mul.lo.s32 	%r39, %r2, %r20;
	shl.b32 	%r40, %r39, 1;
	add.s32 	%r46, %r30, %r40;
	add.s32 	%r47, %r46, 1;
	mul.wide.s32 	%rd7, %r47, 16;
	add.s64 	%rd8, %rd6, %rd7;
	mov.f32 	%f60, 0f3F800000;
	st.global.v4.f32 	[%rd8], {%f20, %f16, %f12, %f60};
	bra.uni 	BB6_4;

BB6_3:
	cvta.to.global.u64 	%rd9, %rd5;
	mul.lo.s32 	%r52, %r2, %r20;
	shl.b32 	%r53, %r52, 1;
	add.s32 	%r59, %r30, %r53;
	add.s32 	%r60, %r59, 1;
	mul.wide.s32 	%rd10, %r60, 8;
	add.s64 	%rd11, %rd9, %rd10;
	mov.f32 	%f61, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f61;
	mov.b16 	%rs1, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f12;
	mov.b16 	%rs2, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f16;
	mov.b16 	%rs3, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f20;
	mov.b16 	%rs4, %temp;
}
	st.global.v4.u16 	[%rd11], {%rs4, %rs3, %rs2, %rs1};

BB6_4:
	mov.b32 	 %f62, %r3;
	mov.b32 	 %f63, %r4;
	mov.b32 	 %f64, %r7;
	mul.ftz.f32 	%f65, %f64, 0f437F0000;
	sub.ftz.f32 	%f66, %f65, %f6;
	mul.ftz.f32 	%f67, %f62, 0f437F0000;
	sub.ftz.f32 	%f68, %f67, %f7;
	mul.ftz.f32 	%f69, %f63, 0f437F0000;
	sub.ftz.f32 	%f70, %f69, %f8;
	mul.ftz.f32 	%f71, %f68, %f10;
	fma.rn.ftz.f32 	%f72, %f66, %f9, %f71;
	fma.rn.ftz.f32 	%f21, %f70, %f11, %f72;
	mul.ftz.f32 	%f73, %f68, %f14;
	fma.rn.ftz.f32 	%f74, %f66, %f13, %f73;
	fma.rn.ftz.f32 	%f22, %f70, %f15, %f74;
	mul.ftz.f32 	%f75, %f68, %f18;
	fma.rn.ftz.f32 	%f76, %f66, %f17, %f75;
	fma.rn.ftz.f32 	%f23, %f70, %f19, %f76;
	shl.b32 	%r70, %r2, 1;
	mad.lo.s32 	%r71, %r70, %r20, %r30;
	cvt.s64.s32	%rd2, %r71;
	@%p4 bra 	BB6_6;

	cvta.to.global.u64 	%rd12, %rd5;
	shl.b64 	%rd13, %rd2, 4;
	add.s64 	%rd14, %rd12, %rd13;
	mov.f32 	%f77, 0f3F800000;
	st.global.v4.f32 	[%rd14], {%f23, %f22, %f21, %f77};
	bra.uni 	BB6_7;

BB6_6:
	cvta.to.global.u64 	%rd15, %rd5;
	shl.b64 	%rd16, %rd2, 3;
	add.s64 	%rd17, %rd15, %rd16;
	mov.f32 	%f78, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f78;
	mov.b16 	%rs5, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f21;
	mov.b16 	%rs6, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f22;
	mov.b16 	%rs7, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f23;
	mov.b16 	%rs8, %temp;
}
	st.global.v4.u16 	[%rd17], {%rs8, %rs7, %rs6, %rs5};

BB6_7:
	add.ftz.f32 	%f79, %f1, 0f3E000000;
	max.ftz.f32 	%f81, %f79, %f31;
	min.ftz.f32 	%f82, %f81, %f2;
	fma.rn.ftz.f32 	%f83, %f82, 0f3F000000, %f2;
	add.ftz.f32 	%f84, %f83, 0f3F000000;
	mov.f32 	%f85, 0f40000000;
	div.approx.ftz.f32 	%f86, %f4, %f85;
	add.ftz.f32 	%f87, %f5, %f86;
	add.ftz.f32 	%f88, %f87, 0f3F000000;
	tex.2d.v4.u32.f32	{%r72, %r73, %r74, %r75}, [inTexture, {%f88, %f84}];
	mov.b32 	 %f89, %r72;
	mov.b32 	 %f90, %r73;
	add.ftz.f32 	%f91, %f3, %f86;
	add.ftz.f32 	%f92, %f91, 0f3F000000;
	tex.2d.v4.u32.f32	{%r11, %r12, %r13, %r14}, [inTexture, {%f92, %f84}];
	tex.2d.v4.u32.f32	{%r15, %r16, %r17, %r18}, [inTexture, {%f92, %f41}];
	add.s32 	%r81, %r70, 1;
	mov.b32 	 %f94, %r16;
	mul.ftz.f32 	%f95, %f90, 0f437F0000;
	mul.ftz.f32 	%f96, %f89, 0f437F0000;
	mul.ftz.f32 	%f97, %f94, 0f437F0000;
	sub.ftz.f32 	%f98, %f97, %f6;
	sub.ftz.f32 	%f99, %f96, %f7;
	sub.ftz.f32 	%f100, %f95, %f8;
	mul.ftz.f32 	%f101, %f99, %f10;
	fma.rn.ftz.f32 	%f102, %f98, %f9, %f101;
	fma.rn.ftz.f32 	%f24, %f100, %f11, %f102;
	mul.ftz.f32 	%f103, %f99, %f14;
	fma.rn.ftz.f32 	%f104, %f98, %f13, %f103;
	fma.rn.ftz.f32 	%f25, %f100, %f15, %f104;
	mul.ftz.f32 	%f105, %f99, %f18;
	fma.rn.ftz.f32 	%f106, %f98, %f17, %f105;
	fma.rn.ftz.f32 	%f26, %f100, %f19, %f106;
	mul.lo.s32 	%r19, %r81, %r20;
	add.s32 	%r87, %r30, %r19;
	add.s32 	%r88, %r87, 1;
	cvt.s64.s32	%rd3, %r88;
	@%p4 bra 	BB6_9;

	cvta.to.global.u64 	%rd18, %rd5;
	shl.b64 	%rd19, %rd3, 4;
	add.s64 	%rd20, %rd18, %rd19;
	mov.f32 	%f107, 0f3F800000;
	st.global.v4.f32 	[%rd20], {%f26, %f25, %f24, %f107};
	bra.uni 	BB6_10;

BB6_9:
	cvta.to.global.u64 	%rd21, %rd5;
	shl.b64 	%rd22, %rd3, 3;
	add.s64 	%rd23, %rd21, %rd22;
	mov.f32 	%f108, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f108;
	mov.b16 	%rs9, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f24;
	mov.b16 	%rs10, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f25;
	mov.b16 	%rs11, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f26;
	mov.b16 	%rs12, %temp;
}
	st.global.v4.u16 	[%rd23], {%rs12, %rs11, %rs10, %rs9};

BB6_10:
	mov.b32 	 %f109, %r11;
	mov.b32 	 %f110, %r12;
	mov.b32 	 %f111, %r15;
	mul.ftz.f32 	%f112, %f111, 0f437F0000;
	sub.ftz.f32 	%f113, %f112, %f6;
	mul.ftz.f32 	%f114, %f109, 0f437F0000;
	sub.ftz.f32 	%f115, %f114, %f7;
	mul.ftz.f32 	%f116, %f110, 0f437F0000;
	sub.ftz.f32 	%f117, %f116, %f8;
	mul.ftz.f32 	%f118, %f115, %f10;
	fma.rn.ftz.f32 	%f119, %f113, %f9, %f118;
	fma.rn.ftz.f32 	%f27, %f117, %f11, %f119;
	mul.ftz.f32 	%f120, %f115, %f14;
	fma.rn.ftz.f32 	%f121, %f113, %f13, %f120;
	fma.rn.ftz.f32 	%f28, %f117, %f15, %f121;
	mul.ftz.f32 	%f122, %f115, %f18;
	fma.rn.ftz.f32 	%f123, %f113, %f17, %f122;
	fma.rn.ftz.f32 	%f29, %f117, %f19, %f123;
	add.s32 	%r94, %r19, %r30;
	cvt.s64.s32	%rd4, %r94;
	@%p4 bra 	BB6_12;

	cvta.to.global.u64 	%rd24, %rd5;
	shl.b64 	%rd25, %rd4, 4;
	add.s64 	%rd26, %rd24, %rd25;
	mov.f32 	%f124, 0f3F800000;
	st.global.v4.f32 	[%rd26], {%f29, %f28, %f27, %f124};
	bra.uni 	BB6_13;

BB6_12:
	cvta.to.global.u64 	%rd27, %rd5;
	shl.b64 	%rd28, %rd4, 3;
	add.s64 	%rd29, %rd27, %rd28;
	mov.f32 	%f125, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f125;
	mov.b16 	%rs13, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f27;
	mov.b16 	%rs14, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f28;
	mov.b16 	%rs15, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f29;
	mov.b16 	%rs16, %temp;
}
	st.global.v4.u16 	[%rd29], {%rs16, %rs15, %rs14, %rs13};

BB6_13:
	ret;
}

	// .globl	PixelFormatConvert_NV12_FRAME_709_FullRange_Kernel
.visible .entry PixelFormatConvert_NV12_FRAME_709_FullRange_Kernel(
	.param .u64 PixelFormatConvert_NV12_FRAME_709_FullRange_Kernel_param_0,
	.param .u64 PixelFormatConvert_NV12_FRAME_709_FullRange_Kernel_param_1,
	.param .u32 PixelFormatConvert_NV12_FRAME_709_FullRange_Kernel_param_2,
	.param .u32 PixelFormatConvert_NV12_FRAME_709_FullRange_Kernel_param_3,
	.param .u32 PixelFormatConvert_NV12_FRAME_709_FullRange_Kernel_param_4,
	.param .u32 PixelFormatConvert_NV12_FRAME_709_FullRange_Kernel_param_5
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<73>;
	.reg .b32 	%r<58>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [PixelFormatConvert_NV12_FRAME_709_FullRange_Kernel_param_1];
	ld.param.u32 	%r11, [PixelFormatConvert_NV12_FRAME_709_FullRange_Kernel_param_2];
	ld.param.u32 	%r12, [PixelFormatConvert_NV12_FRAME_709_FullRange_Kernel_param_3];
	ld.param.u32 	%r13, [PixelFormatConvert_NV12_FRAME_709_FullRange_Kernel_param_4];
	ld.param.u32 	%r14, [PixelFormatConvert_NV12_FRAME_709_FullRange_Kernel_param_5];
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r1, %r16, %r15, %r17;
	mov.u32 	%r18, %ntid.y;
	mov.u32 	%r19, %ctaid.y;
	mov.u32 	%r20, %tid.y;
	mad.lo.s32 	%r2, %r18, %r19, %r20;
	shl.b32 	%r21, %r1, 1;
	setp.ge.u32	%p1, %r21, %r13;
	setp.ge.u32	%p2, %r2, %r14;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB7_7;

	cvt.rn.f32.u32	%f19, %r2;
	add.ftz.f32 	%f20, %f19, 0fBE800000;
	mov.f32 	%f21, 0f00000000;
	max.ftz.f32 	%f22, %f20, %f21;
	cvt.rn.f32.u32	%f23, %r14;
	min.ftz.f32 	%f24, %f22, %f23;
	fma.rn.ftz.f32 	%f25, %f24, 0f3F000000, %f23;
	add.ftz.f32 	%f26, %f25, 0f3F000000;
	cvt.rn.f32.u32	%f27, %r1;
	add.ftz.f32 	%f28, %f27, 0f3F000000;
	cvt.rn.f32.u32	%f29, %r13;
	min.ftz.f32 	%f30, %f28, %f29;
	add.ftz.f32 	%f31, %f30, 0f00000000;
	add.ftz.f32 	%f32, %f31, 0f3F000000;
	tex.2d.v4.u32.f32	{%r22, %r23, %r24, %r25}, [inTexture, {%f32, %f26}];
	mov.b32 	 %f33, %r22;
	mov.b32 	 %f34, %r23;
	tex.2d.v4.u32.f32	{%r3, %r4, %r5, %r6}, [inTexture, {%f28, %f26}];
	add.ftz.f32 	%f35, %f19, 0f3F000000;
	tex.2d.v4.u32.f32	{%r7, %r8, %r9, %r10}, [inTexture, {%f28, %f35}];
	mov.b32 	 %f36, %r8;
	mul.ftz.f32 	%f37, %f34, 0f437F0000;
	mul.ftz.f32 	%f38, %f33, 0f437F0000;
	mul.ftz.f32 	%f39, %f36, 0f437F0000;
	ld.const.f32 	%f40, [kYCbCrFullRangeOffset];
	mov.f32 	%f41, 0f437F0000;
	div.approx.ftz.f32 	%f42, %f41, %f41;
	mul.ftz.f32 	%f1, %f42, %f40;
	sub.ftz.f32 	%f43, %f39, %f1;
	ld.const.f32 	%f44, [kYCbCrFullRangeOffset+4];
	mul.ftz.f32 	%f2, %f42, %f44;
	sub.ftz.f32 	%f45, %f38, %f2;
	ld.const.f32 	%f46, [kYCbCrFullRangeOffset+8];
	mul.ftz.f32 	%f3, %f42, %f46;
	sub.ftz.f32 	%f47, %f37, %f3;
	ld.const.f32 	%f4, [k709YCbCrFullRange_To_RGB32f];
	ld.const.f32 	%f5, [k709YCbCrFullRange_To_RGB32f+4];
	mul.ftz.f32 	%f48, %f45, %f5;
	fma.rn.ftz.f32 	%f49, %f43, %f4, %f48;
	ld.const.f32 	%f6, [k709YCbCrFullRange_To_RGB32f+8];
	fma.rn.ftz.f32 	%f7, %f47, %f6, %f49;
	ld.const.f32 	%f8, [k709YCbCrFullRange_To_RGB32f+12];
	ld.const.f32 	%f9, [k709YCbCrFullRange_To_RGB32f+16];
	mul.ftz.f32 	%f50, %f45, %f9;
	fma.rn.ftz.f32 	%f51, %f43, %f8, %f50;
	ld.const.f32 	%f10, [k709YCbCrFullRange_To_RGB32f+20];
	fma.rn.ftz.f32 	%f11, %f47, %f10, %f51;
	ld.const.f32 	%f12, [k709YCbCrFullRange_To_RGB32f+24];
	ld.const.f32 	%f13, [k709YCbCrFullRange_To_RGB32f+28];
	mul.ftz.f32 	%f52, %f45, %f13;
	fma.rn.ftz.f32 	%f53, %f43, %f12, %f52;
	ld.const.f32 	%f14, [k709YCbCrFullRange_To_RGB32f+32];
	fma.rn.ftz.f32 	%f15, %f47, %f14, %f53;
	setp.eq.s32	%p4, %r12, 0;
	@%p4 bra 	BB7_3;

	cvta.to.global.u64 	%rd4, %rd2;
	mad.lo.s32 	%r35, %r2, %r11, %r21;
	add.s32 	%r36, %r35, 1;
	mul.wide.s32 	%rd5, %r36, 16;
	add.s64 	%rd6, %rd4, %rd5;
	mov.f32 	%f54, 0f3F800000;
	st.global.v4.f32 	[%rd6], {%f15, %f11, %f7, %f54};
	bra.uni 	BB7_4;

BB7_3:
	cvta.to.global.u64 	%rd7, %rd2;
	mad.lo.s32 	%r46, %r2, %r11, %r21;
	add.s32 	%r47, %r46, 1;
	mul.wide.s32 	%rd8, %r47, 8;
	add.s64 	%rd9, %rd7, %rd8;
	mov.f32 	%f55, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f55;
	mov.b16 	%rs1, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f7;
	mov.b16 	%rs2, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f11;
	mov.b16 	%rs3, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f15;
	mov.b16 	%rs4, %temp;
}
	st.global.v4.u16 	[%rd9], {%rs4, %rs3, %rs2, %rs1};

BB7_4:
	mov.b32 	 %f56, %r3;
	mov.b32 	 %f57, %r4;
	mov.b32 	 %f58, %r7;
	mul.ftz.f32 	%f59, %f58, 0f437F0000;
	sub.ftz.f32 	%f60, %f59, %f1;
	mul.ftz.f32 	%f61, %f56, 0f437F0000;
	sub.ftz.f32 	%f62, %f61, %f2;
	mul.ftz.f32 	%f63, %f57, 0f437F0000;
	sub.ftz.f32 	%f64, %f63, %f3;
	mul.ftz.f32 	%f65, %f62, %f5;
	fma.rn.ftz.f32 	%f66, %f60, %f4, %f65;
	fma.rn.ftz.f32 	%f16, %f64, %f6, %f66;
	mul.ftz.f32 	%f67, %f62, %f9;
	fma.rn.ftz.f32 	%f68, %f60, %f8, %f67;
	fma.rn.ftz.f32 	%f17, %f64, %f10, %f68;
	mul.ftz.f32 	%f69, %f62, %f13;
	fma.rn.ftz.f32 	%f70, %f60, %f12, %f69;
	fma.rn.ftz.f32 	%f18, %f64, %f14, %f70;
	mad.lo.s32 	%r57, %r2, %r11, %r21;
	cvt.s64.s32	%rd1, %r57;
	@%p4 bra 	BB7_6;

	cvta.to.global.u64 	%rd10, %rd2;
	shl.b64 	%rd11, %rd1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	mov.f32 	%f71, 0f3F800000;
	st.global.v4.f32 	[%rd12], {%f18, %f17, %f16, %f71};
	bra.uni 	BB7_7;

BB7_6:
	cvta.to.global.u64 	%rd13, %rd2;
	shl.b64 	%rd14, %rd1, 3;
	add.s64 	%rd15, %rd13, %rd14;
	mov.f32 	%f72, 0f3F800000;
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f72;
	mov.b16 	%rs5, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f16;
	mov.b16 	%rs6, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f17;
	mov.b16 	%rs7, %temp;
}
	{
	.reg .b16 %temp;
	cvt.rn.ftz.f16.f32 	%temp, %f18;
	mov.b16 	%rs8, %temp;
}
	st.global.v4.u16 	[%rd15], {%rs8, %rs7, %rs6, %rs5};

BB7_7:
	ret;
}

	// .globl	CopyNV12_Kernel
.visible .entry CopyNV12_Kernel(
	.param .u64 CopyNV12_Kernel_param_0,
	.param .u64 CopyNV12_Kernel_param_1,
	.param .u32 CopyNV12_Kernel_param_2,
	.param .u32 CopyNV12_Kernel_param_3,
	.param .u32 CopyNV12_Kernel_param_4,
	.param .u32 CopyNV12_Kernel_param_5,
	.param .u32 CopyNV12_Kernel_param_6
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [CopyNV12_Kernel_param_0];
	ld.param.u64 	%rd2, [CopyNV12_Kernel_param_1];
	ld.param.u32 	%r3, [CopyNV12_Kernel_param_2];
	ld.param.u32 	%r4, [CopyNV12_Kernel_param_3];
	ld.param.u32 	%r5, [CopyNV12_Kernel_param_4];
	ld.param.u32 	%r7, [CopyNV12_Kernel_param_5];
	ld.param.u32 	%r6, [CopyNV12_Kernel_param_6];
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %ntid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r9, %r8, %r10;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r11, %r12, %r13;
	setp.lt.u32	%p1, %r1, %r5;
	setp.lt.u32	%p2, %r2, %r7;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB8_2;
	bra.uni 	BB8_1;

BB8_1:
	cvta.to.global.u64 	%rd3, %rd1;
	setp.lt.u32	%p4, %r2, %r4;
	selp.b32	%r14, 0, %r6, %p4;
	add.s32 	%r15, %r14, %r2;
	mad.lo.s32 	%r16, %r15, %r3, %r1;
	mul.wide.u32 	%rd4, %r16, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r17, [%rd5];
	mad.lo.s32 	%r18, %r2, %r5, %r1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r18, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r17;

BB8_2:
	ret;
}

	// .globl	NV12FillFail
.visible .entry NV12FillFail(
	.param .u64 NV12FillFail_param_0,
	.param .u32 NV12FillFail_param_1,
	.param .u32 NV12FillFail_param_2,
	.param .u32 NV12FillFail_param_3,
	.param .u32 NV12FillFail_param_4,
	.param .u32 NV12FillFail_param_5
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [NV12FillFail_param_0];
	ld.param.u32 	%r3, [NV12FillFail_param_1];
	ld.param.u32 	%r5, [NV12FillFail_param_3];
	ld.param.u32 	%r4, [NV12FillFail_param_4];
	ld.param.u32 	%r6, [NV12FillFail_param_5];
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r10, %r11, %r12;
	setp.lt.u32	%p1, %r1, %r5;
	setp.lt.u32	%p2, %r2, %r6;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB9_2;
	bra.uni 	BB9_1;

BB9_1:
	cvta.to.global.u64 	%rd2, %rd1;
	mul.lo.s32 	%r13, %r2, %r3;
	cvt.u64.u32	%rd3, %r13;
	shl.b32 	%r14, %r1, 1;
	cvt.u64.u32	%rd4, %r14;
	add.s64 	%rd5, %rd3, %rd4;
	add.s64 	%rd6, %rd2, %rd5;
	setp.lt.u32	%p4, %r2, %r4;
	selp.b16	%rs1, 76, -128, %p4;
	st.global.u8 	[%rd6], %rs1;
	selp.b16	%rs2, 76, -1, %p4;
	st.global.u8 	[%rd6+1], %rs2;

BB9_2:
	ret;
}


